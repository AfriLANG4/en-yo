{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "jw300_word_en_yo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfriLANG4/masakhane/blob/master/jw300_word_en_yo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
        "\n",
        "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "outputId": "4e84d892-38e3-4836-994a-7e5f86e817ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"yo\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "outputId": "dc5694d1-b924-46c5-acec-105daf0e012d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-yo-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "outputId": "e101bb2b-4798-49df-87ec-7acc9bdbc96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.0MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "outputId": "2156f0e6-3f29-4c1c-8f9e-e04a4514cdfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-yo.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   4 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-yo.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "  58 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/yo.zip\n",
            "\n",
            " 325 MB Total size\n",
            "./JW300_latest_xml_en-yo.xml.gz ... 100% of 4 MB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_yo.zip ... 100% of 58 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n48GDRnP8y2G",
        "colab_type": "code",
        "outputId": "0b6c4eb4-8554-4245-a702-560c0ef867c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        }
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-04 11:58:00--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en’\n",
            "\n",
            "\rtest.en-any.en        0%[                    ]       0  --.-KB/s               \rtest.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-03-04 11:58:02 (31.3 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
            "\n",
            "--2020-03-04 11:58:04--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-yo.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 201994 (197K) [text/plain]\n",
            "Saving to: ‘test.en-yo.en’\n",
            "\n",
            "test.en-yo.en       100%[===================>] 197.26K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2020-03-04 11:58:05 (48.9 MB/s) - ‘test.en-yo.en’ saved [201994/201994]\n",
            "\n",
            "--2020-03-04 11:58:10--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-yo.yo\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 280073 (274K) [text/plain]\n",
            "Saving to: ‘test.en-yo.yo’\n",
            "\n",
            "test.en-yo.yo       100%[===================>] 273.51K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-03-04 11:58:11 (26.1 MB/s) - ‘test.en-yo.yo’ saved [280073/280073]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqDG-CI28y2L",
        "colab_type": "code",
        "outputId": "0836dee8-0d8f-4043-b4ff-e7b4d3c27986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "outputId": "47799fae-8a37-4bc0-eecf-32365aef4411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 5663/474986 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Using Ladders — Do You Make These Safety Checks ?</td>\n",
              "      <td>Lílo Àkàbà — Ǹjẹ́ O Máa Ń Ṣe Àyẹ̀wò Wọ̀nyí Tó...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>By Awake !</td>\n",
              "      <td>Látọwọ́ akọ̀ròyìn Jí !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>correspondent in Ireland</td>\n",
              "      <td>ní Ireland</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0  Using Ladders — Do You Make These Safety Checks ?  Lílo Àkàbà — Ǹjẹ́ O Máa Ń Ṣe Àyẹ̀wò Wọ̀nyí Tó...\n",
              "1                                         By Awake !                             Látọwọ́ akọ̀ròyìn Jí !\n",
              "2                           correspondent in Ireland                                         ní Ireland"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "outputId": "22bdc100-534f-4038-cc4c-e7597a97682f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_1BwAApEtMk",
        "colab_type": "code",
        "outputId": "b2cd1ac0-7166-4836-9d5c-0f98a7ecb16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "# NOTE - This might run slow depending on the size of your training set. We are\n",
        "# printing some information to help you track how long it would take. \n",
        "scores = []\n",
        "start_time = time.time()\n",
        "for idx, row in df_pp.iterrows():\n",
        "  scores.append(fuzzfilter(row['source_sentence'], list(en_test_sents), 5))\n",
        "  if idx % 1000 == 0:\n",
        "    hours, rem = divmod(time.time() - start_time, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds), \"%0.2f percent complete\" % (100.0*float(idx)/float(len(df_pp))))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp['scores'] = scores\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (45.1.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144678 sha256=07d282d70b4183259b92853e6a28b88cd6813c597fbae4ee71881651668b32ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "00:00:00.14 0.00 percent complete\n",
            "00:00:20.85 0.24 percent complete\n",
            "00:00:41.32 0.48 percent complete\n",
            "00:01:01.89 0.72 percent complete\n",
            "00:01:23.23 0.96 percent complete\n",
            "00:01:43.88 1.20 percent complete\n",
            "00:02:02.83 1.43 percent complete\n",
            "00:02:23.47 1.67 percent complete\n",
            "00:02:43.59 1.91 percent complete\n",
            "00:03:03.06 2.15 percent complete\n",
            "00:03:22.85 2.39 percent complete\n",
            "00:03:42.14 2.63 percent complete\n",
            "00:04:02.63 2.87 percent complete\n",
            "00:04:22.60 3.11 percent complete\n",
            "00:04:42.74 3.35 percent complete\n",
            "00:05:02.74 3.59 percent complete\n",
            "00:05:22.50 3.82 percent complete\n",
            "00:05:42.35 4.06 percent complete\n",
            "00:06:01.62 4.30 percent complete\n",
            "00:06:21.16 4.54 percent complete\n",
            "00:06:41.27 4.78 percent complete\n",
            "00:07:00.16 5.02 percent complete\n",
            "00:07:19.47 5.26 percent complete\n",
            "00:07:39.39 5.50 percent complete\n",
            "00:07:59.14 5.74 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:08:19.25 5.98 percent complete\n",
            "00:08:39.47 6.22 percent complete\n",
            "00:08:58.63 6.45 percent complete\n",
            "00:09:19.59 6.69 percent complete\n",
            "00:09:38.78 6.93 percent complete\n",
            "00:09:58.31 7.17 percent complete\n",
            "00:10:18.61 7.41 percent complete\n",
            "00:10:38.09 7.65 percent complete\n",
            "00:10:57.08 7.89 percent complete\n",
            "00:11:15.99 8.13 percent complete\n",
            "00:11:34.94 8.37 percent complete\n",
            "00:11:55.10 8.61 percent complete\n",
            "00:12:14.52 8.84 percent complete\n",
            "00:12:33.33 9.08 percent complete\n",
            "00:12:52.75 9.32 percent complete\n",
            "00:13:12.41 9.56 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '” *']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:13:31.66 9.80 percent complete\n",
            "00:13:51.35 10.04 percent complete\n",
            "00:14:10.65 10.28 percent complete\n",
            "00:14:31.20 10.52 percent complete\n",
            "00:14:50.33 10.76 percent complete\n",
            "00:15:09.68 11.00 percent complete\n",
            "00:15:29.14 11.24 percent complete\n",
            "00:15:48.69 11.47 percent complete\n",
            "00:16:07.45 11.71 percent complete\n",
            "00:16:26.30 11.95 percent complete\n",
            "00:16:46.37 12.19 percent complete\n",
            "00:17:07.50 12.43 percent complete\n",
            "00:17:26.35 12.67 percent complete\n",
            "00:17:45.63 12.91 percent complete\n",
            "00:18:04.18 13.15 percent complete\n",
            "00:18:23.36 13.39 percent complete\n",
            "00:18:42.56 13.63 percent complete\n",
            "00:19:01.18 13.86 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '. .']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:19:20.38 14.10 percent complete\n",
            "00:19:40.13 14.34 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:19:58.60 14.58 percent complete\n",
            "00:20:17.48 14.82 percent complete\n",
            "00:20:36.16 15.06 percent complete\n",
            "00:20:54.38 15.30 percent complete\n",
            "00:21:13.29 15.54 percent complete\n",
            "00:21:32.15 15.78 percent complete\n",
            "00:21:51.94 16.02 percent complete\n",
            "00:22:11.54 16.26 percent complete\n",
            "00:22:30.31 16.49 percent complete\n",
            "00:22:49.26 16.73 percent complete\n",
            "00:23:08.27 16.97 percent complete\n",
            "00:23:26.97 17.21 percent complete\n",
            "00:23:46.13 17.45 percent complete\n",
            "00:24:04.14 17.69 percent complete\n",
            "00:24:22.96 17.93 percent complete\n",
            "00:24:42.30 18.17 percent complete\n",
            "00:25:01.13 18.41 percent complete\n",
            "00:25:20.07 18.65 percent complete\n",
            "00:25:38.77 18.88 percent complete\n",
            "00:25:56.85 19.12 percent complete\n",
            "00:26:15.64 19.36 percent complete\n",
            "00:26:34.89 19.60 percent complete\n",
            "00:26:53.65 19.84 percent complete\n",
            "00:27:12.29 20.08 percent complete\n",
            "00:27:31.43 20.32 percent complete\n",
            "00:27:50.16 20.56 percent complete\n",
            "00:28:09.30 20.80 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:28:28.12 21.04 percent complete\n",
            "00:28:47.50 21.28 percent complete\n",
            "00:29:06.41 21.51 percent complete\n",
            "00:29:25.17 21.75 percent complete\n",
            "00:29:43.83 21.99 percent complete\n",
            "00:30:03.47 22.23 percent complete\n",
            "00:30:22.99 22.47 percent complete\n",
            "00:30:41.67 22.71 percent complete\n",
            "00:31:00.48 22.95 percent complete\n",
            "00:31:19.30 23.19 percent complete\n",
            "00:31:37.55 23.43 percent complete\n",
            "00:31:56.32 23.67 percent complete\n",
            "00:32:15.07 23.90 percent complete\n",
            "00:32:35.33 24.14 percent complete\n",
            "00:32:54.01 24.38 percent complete\n",
            "00:33:12.78 24.62 percent complete\n",
            "00:33:31.14 24.86 percent complete\n",
            "00:33:49.96 25.10 percent complete\n",
            "00:34:08.74 25.34 percent complete\n",
            "00:34:27.37 25.58 percent complete\n",
            "00:34:46.20 25.82 percent complete\n",
            "00:35:06.13 26.06 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '→ →']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:35:24.30 26.29 percent complete\n",
            "00:35:43.69 26.53 percent complete\n",
            "00:36:02.81 26.77 percent complete\n",
            "00:36:21.75 27.01 percent complete\n",
            "00:36:40.61 27.25 percent complete\n",
            "00:36:59.35 27.49 percent complete\n",
            "00:37:18.47 27.73 percent complete\n",
            "00:37:39.00 27.97 percent complete\n",
            "00:37:57.55 28.21 percent complete\n",
            "00:38:16.57 28.45 percent complete\n",
            "00:38:35.34 28.69 percent complete\n",
            "00:38:53.92 28.92 percent complete\n",
            "00:39:12.64 29.16 percent complete\n",
            "00:39:31.48 29.40 percent complete\n",
            "00:39:50.01 29.64 percent complete\n",
            "00:40:09.48 29.88 percent complete\n",
            "00:40:28.01 30.12 percent complete\n",
            "00:40:47.01 30.36 percent complete\n",
            "00:41:06.24 30.60 percent complete\n",
            "00:41:25.34 30.84 percent complete\n",
            "00:41:44.72 31.08 percent complete\n",
            "00:42:03.30 31.31 percent complete\n",
            "00:42:22.52 31.55 percent complete\n",
            "00:42:42.98 31.79 percent complete\n",
            "00:43:01.69 32.03 percent complete\n",
            "00:43:21.08 32.27 percent complete\n",
            "00:43:40.22 32.51 percent complete\n",
            "00:43:58.76 32.75 percent complete\n",
            "00:44:17.96 32.99 percent complete\n",
            "00:44:36.70 33.23 percent complete\n",
            "00:44:54.83 33.47 percent complete\n",
            "00:45:14.85 33.71 percent complete\n",
            "00:45:33.37 33.94 percent complete\n",
            "00:45:52.14 34.18 percent complete\n",
            "00:46:10.77 34.42 percent complete\n",
            "00:46:29.29 34.66 percent complete\n",
            "00:46:48.12 34.90 percent complete\n",
            "00:47:07.25 35.14 percent complete\n",
            "00:47:25.50 35.38 percent complete\n",
            "00:47:45.82 35.62 percent complete\n",
            "00:48:04.83 35.86 percent complete\n",
            "00:48:23.73 36.10 percent complete\n",
            "00:48:42.85 36.33 percent complete\n",
            "00:49:00.76 36.57 percent complete\n",
            "00:49:19.48 36.81 percent complete\n",
            "00:49:38.74 37.05 percent complete\n",
            "00:49:56.83 37.29 percent complete\n",
            "00:50:17.18 37.53 percent complete\n",
            "00:50:37.18 37.77 percent complete\n",
            "00:50:55.00 38.01 percent complete\n",
            "00:51:13.96 38.25 percent complete\n",
            "00:51:32.05 38.49 percent complete\n",
            "00:51:50.90 38.73 percent complete\n",
            "00:52:09.63 38.96 percent complete\n",
            "00:52:28.39 39.20 percent complete\n",
            "00:52:47.53 39.44 percent complete\n",
            "00:53:07.65 39.68 percent complete\n",
            "00:53:26.11 39.92 percent complete\n",
            "00:53:45.06 40.16 percent complete\n",
            "00:54:03.88 40.40 percent complete\n",
            "00:54:22.19 40.64 percent complete\n",
            "00:54:41.29 40.88 percent complete\n",
            "00:54:59.95 41.12 percent complete\n",
            "00:55:19.24 41.35 percent complete\n",
            "00:55:39.42 41.59 percent complete\n",
            "00:55:57.84 41.83 percent complete\n",
            "00:56:16.88 42.07 percent complete\n",
            "00:56:35.17 42.31 percent complete\n",
            "00:56:53.86 42.55 percent complete\n",
            "00:57:13.24 42.79 percent complete\n",
            "00:57:32.16 43.03 percent complete\n",
            "00:57:50.75 43.27 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓ ↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:58:10.82 43.51 percent complete\n",
            "00:58:29.63 43.75 percent complete\n",
            "00:58:48.44 43.98 percent complete\n",
            "00:59:06.64 44.22 percent complete\n",
            "00:59:25.44 44.46 percent complete\n",
            "00:59:44.60 44.70 percent complete\n",
            "01:00:02.99 44.94 percent complete\n",
            "01:00:22.00 45.18 percent complete\n",
            "01:00:41.55 45.42 percent complete\n",
            "01:00:59.83 45.66 percent complete\n",
            "01:01:18.91 45.90 percent complete\n",
            "01:01:37.43 46.14 percent complete\n",
            "01:01:55.61 46.37 percent complete\n",
            "01:02:14.65 46.61 percent complete\n",
            "01:02:33.42 46.85 percent complete\n",
            "01:02:51.83 47.09 percent complete\n",
            "01:03:12.79 47.33 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '*']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:03:31.39 47.57 percent complete\n",
            "01:03:50.52 47.81 percent complete\n",
            "01:04:09.35 48.05 percent complete\n",
            "01:04:28.14 48.29 percent complete\n",
            "01:04:47.38 48.53 percent complete\n",
            "01:05:06.49 48.77 percent complete\n",
            "01:05:25.25 49.00 percent complete\n",
            "01:05:45.08 49.24 percent complete\n",
            "01:06:03.21 49.48 percent complete\n",
            "01:06:22.33 49.72 percent complete\n",
            "01:06:41.61 49.96 percent complete\n",
            "01:06:59.46 50.20 percent complete\n",
            "01:07:18.24 50.44 percent complete\n",
            "01:07:36.87 50.68 percent complete\n",
            "01:07:55.29 50.92 percent complete\n",
            "01:08:15.02 51.16 percent complete\n",
            "01:08:33.72 51.39 percent complete\n",
            "01:08:52.45 51.63 percent complete\n",
            "01:09:10.60 51.87 percent complete\n",
            "01:09:29.30 52.11 percent complete\n",
            "01:09:48.75 52.35 percent complete\n",
            "01:10:08.26 52.59 percent complete\n",
            "01:10:27.14 52.83 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇩']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:10:47.42 53.07 percent complete\n",
            "01:11:06.03 53.31 percent complete\n",
            "01:11:24.45 53.55 percent complete\n",
            "01:11:43.42 53.79 percent complete\n",
            "01:12:02.21 54.02 percent complete\n",
            "01:12:21.04 54.26 percent complete\n",
            "01:12:40.02 54.50 percent complete\n",
            "01:12:58.62 54.74 percent complete\n",
            "01:13:18.93 54.98 percent complete\n",
            "01:13:37.89 55.22 percent complete\n",
            "01:13:56.04 55.46 percent complete\n",
            "01:14:14.78 55.70 percent complete\n",
            "01:14:33.34 55.94 percent complete\n",
            "01:14:51.96 56.18 percent complete\n",
            "01:15:10.56 56.41 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '”']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:15:29.66 56.65 percent complete\n",
            "01:15:49.73 56.89 percent complete\n",
            "01:16:08.34 57.13 percent complete\n",
            "01:16:26.50 57.37 percent complete\n",
            "01:16:45.64 57.61 percent complete\n",
            "01:17:04.12 57.85 percent complete\n",
            "01:17:22.61 58.09 percent complete\n",
            "01:17:41.55 58.33 percent complete\n",
            "01:17:59.97 58.57 percent complete\n",
            "01:18:20.48 58.81 percent complete\n",
            "01:18:40.15 59.04 percent complete\n",
            "01:18:58.93 59.28 percent complete\n",
            "01:19:17.81 59.52 percent complete\n",
            "01:19:36.67 59.76 percent complete\n",
            "01:19:54.72 60.00 percent complete\n",
            "01:20:13.82 60.24 percent complete\n",
            "01:20:31.98 60.48 percent complete\n",
            "01:20:51.43 60.72 percent complete\n",
            "01:21:11.23 60.96 percent complete\n",
            "01:21:30.17 61.20 percent complete\n",
            "01:21:49.13 61.43 percent complete\n",
            "01:22:07.15 61.67 percent complete\n",
            "01:22:25.89 61.91 percent complete\n",
            "01:22:45.38 62.15 percent complete\n",
            "01:23:04.54 62.39 percent complete\n",
            "01:23:23.67 62.63 percent complete\n",
            "01:23:43.28 62.87 percent complete\n",
            "01:24:01.91 63.11 percent complete\n",
            "01:24:20.87 63.35 percent complete\n",
            "01:24:39.89 63.59 percent complete\n",
            "01:24:58.39 63.83 percent complete\n",
            "01:25:17.34 64.06 percent complete\n",
            "01:25:36.60 64.30 percent complete\n",
            "01:25:55.17 64.54 percent complete\n",
            "01:26:15.10 64.78 percent complete\n",
            "01:26:34.00 65.02 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓ ↓ ↓ ↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:26:52.54 65.26 percent complete\n",
            "01:27:10.88 65.50 percent complete\n",
            "01:27:29.05 65.74 percent complete\n",
            "01:27:48.47 65.98 percent complete\n",
            "01:28:07.01 66.22 percent complete\n",
            "01:28:25.75 66.45 percent complete\n",
            "01:28:45.29 66.69 percent complete\n",
            "01:29:03.87 66.93 percent complete\n",
            "01:29:22.75 67.17 percent complete\n",
            "01:29:41.61 67.41 percent complete\n",
            "01:29:59.81 67.65 percent complete\n",
            "01:30:19.00 67.89 percent complete\n",
            "01:30:37.34 68.13 percent complete\n",
            "01:30:56.21 68.37 percent complete\n",
            "01:31:16.26 68.61 percent complete\n",
            "01:31:34.75 68.85 percent complete\n",
            "01:31:53.36 69.08 percent complete\n",
            "01:32:12.26 69.32 percent complete\n",
            "01:32:30.98 69.56 percent complete\n",
            "01:32:49.35 69.80 percent complete\n",
            "01:33:07.59 70.04 percent complete\n",
            "01:33:26.10 70.28 percent complete\n",
            "01:33:46.93 70.52 percent complete\n",
            "01:34:05.58 70.76 percent complete\n",
            "01:34:24.56 71.00 percent complete\n",
            "01:34:43.62 71.24 percent complete\n",
            "01:35:02.52 71.47 percent complete\n",
            "01:35:20.95 71.71 percent complete\n",
            "01:35:39.64 71.95 percent complete\n",
            "01:35:58.31 72.19 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '\\']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:36:19.09 72.43 percent complete\n",
            "01:36:37.85 72.67 percent complete\n",
            "01:36:56.24 72.91 percent complete\n",
            "01:37:15.30 73.15 percent complete\n",
            "01:37:33.93 73.39 percent complete\n",
            "01:37:52.74 73.63 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '●']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:38:11.77 73.86 percent complete\n",
            "01:38:30.59 74.10 percent complete\n",
            "01:38:50.61 74.34 percent complete\n",
            "01:39:09.35 74.58 percent complete\n",
            "01:39:28.65 74.82 percent complete\n",
            "01:39:47.47 75.06 percent complete\n",
            "01:40:06.00 75.30 percent complete\n",
            "01:40:24.66 75.54 percent complete\n",
            "01:40:43.45 75.78 percent complete\n",
            "01:41:01.39 76.02 percent complete\n",
            "01:41:21.51 76.26 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:41:40.71 76.49 percent complete\n",
            "01:41:59.56 76.73 percent complete\n",
            "01:42:18.36 76.97 percent complete\n",
            "01:42:37.39 77.21 percent complete\n",
            "01:42:56.60 77.45 percent complete\n",
            "01:43:15.66 77.69 percent complete\n",
            "01:43:34.35 77.93 percent complete\n",
            "01:43:54.87 78.17 percent complete\n",
            "01:44:14.09 78.41 percent complete\n",
            "01:44:33.11 78.65 percent complete\n",
            "01:44:52.13 78.88 percent complete\n",
            "01:45:10.10 79.12 percent complete\n",
            "01:45:29.01 79.36 percent complete\n",
            "01:45:48.31 79.60 percent complete\n",
            "01:46:06.89 79.84 percent complete\n",
            "01:46:26.03 80.08 percent complete\n",
            "01:46:44.92 80.32 percent complete\n",
            "01:47:03.70 80.56 percent complete\n",
            "01:47:22.44 80.80 percent complete\n",
            "01:47:41.34 81.04 percent complete\n",
            "01:47:59.39 81.28 percent complete\n",
            "01:48:18.49 81.51 percent complete\n",
            "01:48:36.85 81.75 percent complete\n",
            "01:48:55.54 81.99 percent complete\n",
            "01:49:15.32 82.23 percent complete\n",
            "01:49:34.01 82.47 percent complete\n",
            "01:49:52.54 82.71 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '□ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:50:11.80 82.95 percent complete\n",
            "01:50:30.24 83.19 percent complete\n",
            "01:50:49.24 83.43 percent complete\n",
            "01:51:07.99 83.67 percent complete\n",
            "01:51:27.13 83.90 percent complete\n",
            "01:51:47.59 84.14 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '— ― ― ― ― ― ― ―']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:52:06.15 84.38 percent complete\n",
            "01:52:24.81 84.62 percent complete\n",
            "01:52:43.69 84.86 percent complete\n",
            "01:53:02.09 85.10 percent complete\n",
            "01:53:21.38 85.34 percent complete\n",
            "01:53:40.35 85.58 percent complete\n",
            "01:53:59.41 85.82 percent complete\n",
            "01:54:19.05 86.06 percent complete\n",
            "01:54:38.43 86.30 percent complete\n",
            "01:54:56.96 86.53 percent complete\n",
            "01:55:15.83 86.77 percent complete\n",
            "01:55:34.57 87.01 percent complete\n",
            "01:55:53.20 87.25 percent complete\n",
            "01:56:11.65 87.49 percent complete\n",
            "01:56:30.15 87.73 percent complete\n",
            "01:56:50.36 87.97 percent complete\n",
            "01:57:08.96 88.21 percent complete\n",
            "01:57:27.22 88.45 percent complete\n",
            "01:57:46.42 88.69 percent complete\n",
            "01:58:04.50 88.92 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇧']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:58:23.13 89.16 percent complete\n",
            "01:58:42.12 89.40 percent complete\n",
            "01:59:00.71 89.64 percent complete\n",
            "01:59:21.36 89.88 percent complete\n",
            "01:59:40.28 90.12 percent complete\n",
            "01:59:59.01 90.36 percent complete\n",
            "02:00:17.40 90.60 percent complete\n",
            "02:00:36.56 90.84 percent complete\n",
            "02:00:54.90 91.08 percent complete\n",
            "02:01:13.23 91.32 percent complete\n",
            "02:01:31.90 91.55 percent complete\n",
            "02:01:52.05 91.79 percent complete\n",
            "02:02:10.58 92.03 percent complete\n",
            "02:02:29.18 92.27 percent complete\n",
            "02:02:48.67 92.51 percent complete\n",
            "02:03:07.35 92.75 percent complete\n",
            "02:03:26.58 92.99 percent complete\n",
            "02:03:45.28 93.23 percent complete\n",
            "02:04:04.13 93.47 percent complete\n",
            "02:04:24.56 93.71 percent complete\n",
            "02:04:43.53 93.94 percent complete\n",
            "02:05:02.01 94.18 percent complete\n",
            "02:05:20.47 94.42 percent complete\n",
            "02:05:39.71 94.66 percent complete\n",
            "02:05:58.13 94.90 percent complete\n",
            "02:06:17.26 95.14 percent complete\n",
            "02:06:35.60 95.38 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:06:55.85 95.62 percent complete\n",
            "02:07:14.40 95.86 percent complete\n",
            "02:07:32.57 96.10 percent complete\n",
            "02:07:51.41 96.34 percent complete\n",
            "02:08:09.85 96.57 percent complete\n",
            "02:08:28.32 96.81 percent complete\n",
            "02:08:47.16 97.05 percent complete\n",
            "02:09:05.13 97.29 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:09:24.48 97.53 percent complete\n",
            "02:09:43.69 97.77 percent complete\n",
            "02:10:02.11 98.01 percent complete\n",
            "02:10:20.52 98.25 percent complete\n",
            "02:10:39.09 98.49 percent complete\n",
            "02:10:57.11 98.73 percent complete\n",
            "02:11:16.25 98.96 percent complete\n",
            "02:11:34.55 99.20 percent complete\n",
            "02:11:54.78 99.44 percent complete\n",
            "02:12:14.20 99.68 percent complete\n",
            "02:12:32.73 99.92 percent complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHoiifVa0nKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Second Run Move data from drive to \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "outputId": "b1eff87f-f76b-4f38-ede8-3225db762965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "TRAIN YOUR CHILDREN : “ I teach my children to check the expiration date of any packaged food items , such as snacks , before they buy them . ” ​ — Ruth , Nigeria\n",
            "When she replied that she was , he explained that he and his mother were trying to assist his sister with a school report on Canadians .\n",
            "Through the prophet Zephaniah , Jehovah answers : “ That day is a day of fury , a day of distress and of anguish , a day of storm and of desolation , a day of darkness and of gloominess , a day of clouds and of thick gloom . ”\n",
            "We do not require that people simply do as we tell them , but we give them convincing reasons to obey Christ’s command .\n",
            "Still , Jehovah can annihilate any rebel in the lake of fire , denying him any hope of a resurrection .\n",
            "Lucaris was arrested , and on July 27 , 1638 , he was taken on board a small boat as if for banishment .\n",
            "Yes , Jehovah remembered their faithful course .\n",
            "□ To appear tough\n",
            "During the ensuing confrontation , we Witnesses had to make our position clear to the agitated rebels and also explain our stand to the military guards .\n",
            "( See opening image . ) ( c ) Why should this Bible account about Samuel be of special interest to elders today ?\n",
            "\n",
            "==> train.yo <==\n",
            "KỌ́ ÀWỌN ỌMỌ RẸ : “ Mo kọ́ àwọn ọmọ mi pé kí wọ́n tó ra oúnjẹ bí ìpápánu , tó wà nínú agolo , ike , bébà , tàbí ọ̀rá , kí wọ́n máa yẹ ara oúnjẹ náà wò kí wọ́n lè mọ déètì tó máa bà jẹ́ . ” — Ruth , Nàìjíríà\n",
            "Nígbà tó sọ fún ọ̀dọ́kùnrin náà pé Kánádà lòun ti wá , ọ̀dọ́kùnrin náà sọ fún un pé òun àti màmá òun fẹ́ ran àbúrò òun obìnrin kan lọ́wọ́ láti kó ọ̀rọ̀ kan jọ nípa àwọn ará Kánádà , èyí tó fẹ́ mu lọ sílé ìwé .\n",
            "Jèhófà gbẹnu wòlíì Sefanáyà sọ ìdí rẹ̀ fún wa , ó ní : “ Ọjọ́ yẹn jẹ́ ọjọ́ ìbínú kíkan , ọjọ́ wàhálà àti làásìgbò , ọjọ́ ìjì àti ìsọdahoro , ọjọ́ òkùnkùn àti ìṣúdùdù , ọjọ́ àwọsánmà àti ìṣúdùdù tí ó nípọn . ”\n",
            "A ò fẹ́ káwọn èèyàn wulẹ̀ ṣe ohun tá a sọ fún wọn nìkan , àmọ́ à tún ń fún wọn ní ẹ̀rí tó dájú nípa ìdí tó fi yẹ kí wọ́n ṣègbọràn sí àṣẹ Kristi .\n",
            "Síbẹ̀ , Jèhófà lè pa ọlọ̀tẹ̀ èyíkéyìí run nípa sísọ ọ sínú adágún iná , tó túmọ̀ sí pé onítọ̀hún máa kú láìsí ìrètí kankan láti tún jíǹde .\n",
            "Ní wọ́n bá fi ọlọ́pàá mú Lucaris , nígbà tó sì di July 27 , 1638 , wọ́n fi ọkọ̀ ojú omi wà á lọ bí ẹni pé wọ́n fẹ́ gbé e lọ sí ilẹ̀ mìíràn .\n",
            "Bẹ́ẹ̀ ni o , Jèhófà kò gbàgbé ìṣòtítọ́ wọn .\n",
            "□ Kí wọ́n má bàa fojú ọ̀dẹ̀ wò mí\n",
            "Lákòókò tí àríyànjiyàn náà ń lọ lọ́wọ́ , àwa Ẹlẹ́rìí jẹ́ káwọn ọlọ̀tẹ̀ tínú ń bí yìí mọ̀ pé a ò lọ́wọ́ sí nǹkan tí wọ́n ń ṣe , a sì tún ṣàlàyé irú ẹni tá a jẹ́ fáwọn ológun tó ń ṣọ́ ọgbà ẹ̀wọ̀n náà .\n",
            "( Wo àwòrán tó wà níbẹ̀rẹ̀ àpilẹ̀kọ yìí . ) ( d ) Kí nìdí tó fi yẹ kí àwọn alàgbà lóde òní fún àkọsílẹ̀ Bíbélì yìí nípa Sámúẹ́lì láfiyèsí àrà ọ̀tọ̀ ?\n",
            "==> dev.en <==\n",
            "He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "Now I had to find a legitimate line of work .\n",
            "Do I value material things more than my relationship with Jehovah and with people ?\n",
            "He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "According to Harkavy’s Students ’ Hebrew and Chaldee Dictionary , ʽadh means “ duration , everlastingness , eternity , for ever . ”\n",
            "Why is rendering proper honor to elders a concern ?\n",
            "Jeremiah would rather be alone than be corrupted by bad companions .\n",
            "In years gone by , we believed that Jehovah became displeased with his people because they did not have a zealous share in the preaching work during World War I .\n",
            "Rather , they need to use a translation of the Bible in their own language .\n",
            "On a more personal level , showing honor to those to whom it is due keeps us from becoming self - centered .\n",
            "\n",
            "==> dev.yo <==\n",
            "Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "Gẹ́gẹ́ bí Harkavy’s Students ’ Hebrew and Chaldee Dictionary ṣe sọ , ʽadh túmọ̀ sí “ àkókò gígùn , àìnípẹ̀kun , títí gbére , títí láé . ”\n",
            "Kí nìdí tí kò fi yẹ ká máa gbé àwọn alàgbà gẹ̀gẹ̀ ju bó ṣe yẹ lọ ?\n",
            "Jeremáyà gbà kóun dá wà ju pé káwọn ọ̀rẹ́ burúkú wá kéèràn ran òun .\n",
            "Láwọn ọdún mélòó kan sẹ́yìn , a gbà pé inú Jèhófà ò dùn sáwọn èèyàn rẹ̀ torí pé wọn ò fìtara wàásù lásìkò Ogun Àgbáyé Kìíní .\n",
            "Àfi kí wọ́n ka Bíbélì tí wọ́n tú sí èdè wọn .\n",
            "Tó bá ti mọ́ wa lára láti máa bọlá fáwọn míì , a ò ní máa ro tara wa nìkan .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiub_CB7uvIf",
        "colab_type": "code",
        "outputId": "c8c03119-8662-4c9c-e900-f61af5dcabc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "! ls -lah\n",
        "! cat train.en | wc -l\n",
        "! cat train.yo | wc -l\n",
        "! cat dev.en | wc -l\n",
        "! cat dev.yo | wc -l\n",
        "! cat test.en | wc -l\n",
        "! cat test.yo | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 433M\n",
            "-rw------- 1 root root  93K Mar  4 14:29 dev.en\n",
            "-rw------- 1 root root 135K Mar  4 14:29 dev.yo\n",
            "-rw------- 1 root root  27M Mar  4 11:55 JW300_latest_xml_en-yo.xml\n",
            "-rw------- 1 root root 257M Mar  4 11:55 JW300_latest_xml_en.zip\n",
            "-rw------- 1 root root  57M Mar  4 11:56 JW300_latest_xml_yo.zip\n",
            "drwx------ 3 root root 4.0K Mar  4 14:35 models\n",
            "-rw------- 1 root root 198K Mar  4 14:29 test.en\n",
            "-rw------- 1 root root 272K Mar  4 14:29 test.en-any.en\n",
            "-rw------- 1 root root 274K Mar  4 14:29 test.yo\n",
            "-rw------- 1 root root  38M Mar  4 14:29 train.en\n",
            "-rw------- 1 root root  55M Mar  4 14:29 train.yo\n",
            "-rw------- 1 root root 3.3K Mar  4 14:42 transformer_enyo.yaml\n",
            "-rw------- 1 root root 911K Mar  4 14:30 vocab.txt\n",
            "415801\n",
            "415801\n",
            "1000\n",
            "1000\n",
            "2665\n",
            "2665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {}
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "outputId": "e0cc9164-bf67-4f22-a9a9-9f5580bea6e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# # One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# # Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# # Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "# ! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# # Apply BPE splits to the development and test data.\n",
        "# ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "# ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "# ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "# ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "# ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "# ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "#! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "# ! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.$src joeynmt/data/$src$tgt/train.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"Word Yorùbá Sentences\"\n",
        "! tail -n 5 test.$tgt\n",
        "! echo \"Combined Word Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n",
        "! cp joeynmt/data/$src$tgt/vocab.txt \"$gdrive_path\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.en\tdev.yo\ttest.en  test.yo  train.en  train.yo  vocab.txt\n",
            "Word Yorùbá Sentences\n",
            "tail: cannot open 'test.yo' for reading: No such file or directory\n",
            "Combined Word Vocab\n",
            "PERCENTAGE\n",
            "gated\n",
            "generalizations\n",
            "Allegory\n",
            "drenching\n",
            "BURN\n",
            "antiretrovirals\n",
            "hurtfulness\n",
            "deification\n",
            "Àìsíwèé\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OkAKSIK7Eg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ###### IOHAVOC MODIFICATIONS ==>> CREATE THE VOCAB FOR NON-BPE EXPERIMENTS\n",
        "# from os import path\n",
        "\n",
        "# os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "# os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# ! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "# ! joeynmt/scripts/build_vocab.py \"$gdrive_path/train.$src\" \"$gdrive_path/train.$tgt\" --output_path \"$gdrive_path/vocab-nonBPE.txt\"\n",
        "\n",
        "# # Some output\n",
        "# ! echo \"Yorùbá test Sentences\"\n",
        "# ! tail -n 5 \"$gdrive_path/test.$tgt\"\n",
        "# ! echo \"Combined Vocab\"\n",
        "# ! tail -n 10 \"$gdrive_path/vocab-nonBPE.txt\"  # iroro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nR4hpHL-nTJ",
        "colab_type": "code",
        "outputId": "521e8a74-fae2-4c41-d045-f8f6779b44fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#Checked which CUDA version is installed\n",
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpd91xek-upl",
        "colab_type": "code",
        "outputId": "a640474c-697e-49b5-cbff-06d4dafbf752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Enabled persistence mode for GPU \n",
        "# Enable persistent mode ...\n",
        "! nvidia-smi -pm 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enabled persistence mode for GPU 00000000:00:04.0.\n",
            "All done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"{gdrive_path}/train\"\n",
        "    dev:   \"{gdrive_path}/dev\"\n",
        "    test:  \"{gdrive_path}/test\"\n",
        "    level: \"word\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"{gdrive_path}/vocab.txt\"\n",
        "    trg_vocab: \"{gdrive_path}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    # load_model: \"{gdrive_path}/models/{name}_transformer_orig/142000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 3600        # Decrease from 4096\n",
        "    batch_type: \"token\"     # May likely change to sentence\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 100                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"{gdrive_path}/models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "! cp joeynmt/configs/transformer_$src$tgt.yaml \"$gdrive_path\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "outputId": "7e1fbab0-ec7d-45fb-a3d1-c0ac3599fccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "# !cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml\n",
        "!python3 -m joeynmt train \"$gdrive_path/transformer_$src$tgt.yaml\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-04 14:44:35,831 Hello! This is Joey-NMT.\n",
            "2020-03-04 14:44:36,951 Total params: 36540672\n",
            "2020-03-04 14:44:36,953 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2020-03-04 14:44:41,078 cfg.name                           : enyo_transformer\n",
            "2020-03-04 14:44:41,078 cfg.data.src                       : en\n",
            "2020-03-04 14:44:41,078 cfg.data.trg                       : yo\n",
            "2020-03-04 14:44:41,079 cfg.data.train                     : /content/drive/My Drive/masakhane/en-yo-baseline/train\n",
            "2020-03-04 14:44:41,079 cfg.data.dev                       : /content/drive/My Drive/masakhane/en-yo-baseline/dev\n",
            "2020-03-04 14:44:41,079 cfg.data.test                      : /content/drive/My Drive/masakhane/en-yo-baseline/test\n",
            "2020-03-04 14:44:41,079 cfg.data.level                     : word\n",
            "2020-03-04 14:44:41,079 cfg.data.lowercase                 : False\n",
            "2020-03-04 14:44:41,079 cfg.data.max_sent_length           : 100\n",
            "2020-03-04 14:44:41,079 cfg.data.src_vocab                 : /content/drive/My Drive/masakhane/en-yo-baseline/vocab.txt\n",
            "2020-03-04 14:44:41,079 cfg.data.trg_vocab                 : /content/drive/My Drive/masakhane/en-yo-baseline/vocab.txt\n",
            "2020-03-04 14:44:41,079 cfg.testing.beam_size              : 5\n",
            "2020-03-04 14:44:41,080 cfg.testing.alpha                  : 1.0\n",
            "2020-03-04 14:44:41,080 cfg.training.random_seed           : 42\n",
            "2020-03-04 14:44:41,080 cfg.training.optimizer             : adam\n",
            "2020-03-04 14:44:41,080 cfg.training.normalization         : tokens\n",
            "2020-03-04 14:44:41,080 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2020-03-04 14:44:41,080 cfg.training.scheduling            : plateau\n",
            "2020-03-04 14:44:41,080 cfg.training.patience              : 5\n",
            "2020-03-04 14:44:41,081 cfg.training.learning_rate_factor  : 0.5\n",
            "2020-03-04 14:44:41,081 cfg.training.learning_rate_warmup  : 1000\n",
            "2020-03-04 14:44:41,081 cfg.training.decrease_factor       : 0.7\n",
            "2020-03-04 14:44:41,081 cfg.training.loss                  : crossentropy\n",
            "2020-03-04 14:44:41,081 cfg.training.learning_rate         : 0.0003\n",
            "2020-03-04 14:44:41,081 cfg.training.learning_rate_min     : 1e-08\n",
            "2020-03-04 14:44:41,081 cfg.training.weight_decay          : 0.0\n",
            "2020-03-04 14:44:41,081 cfg.training.label_smoothing       : 0.1\n",
            "2020-03-04 14:44:41,081 cfg.training.batch_size            : 3600\n",
            "2020-03-04 14:44:41,082 cfg.training.batch_type            : token\n",
            "2020-03-04 14:44:41,082 cfg.training.eval_batch_size       : 3600\n",
            "2020-03-04 14:44:41,082 cfg.training.eval_batch_type       : token\n",
            "2020-03-04 14:44:41,082 cfg.training.batch_multiplier      : 1\n",
            "2020-03-04 14:44:41,082 cfg.training.early_stopping_metric : ppl\n",
            "2020-03-04 14:44:41,082 cfg.training.epochs                : 100\n",
            "2020-03-04 14:44:41,082 cfg.training.validation_freq       : 1000\n",
            "2020-03-04 14:44:41,082 cfg.training.logging_freq          : 100\n",
            "2020-03-04 14:44:41,083 cfg.training.eval_metric           : bleu\n",
            "2020-03-04 14:44:41,083 cfg.training.model_dir             : /content/drive/My Drive/masakhane/en-yo-baseline/models/enyo_transformer\n",
            "2020-03-04 14:44:41,083 cfg.training.overwrite             : True\n",
            "2020-03-04 14:44:41,083 cfg.training.shuffle               : True\n",
            "2020-03-04 14:44:41,083 cfg.training.use_cuda              : True\n",
            "2020-03-04 14:44:41,083 cfg.training.max_output_length     : 100\n",
            "2020-03-04 14:44:41,083 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2020-03-04 14:44:41,083 cfg.training.keep_last_ckpts       : 3\n",
            "2020-03-04 14:44:41,084 cfg.model.initializer              : xavier\n",
            "2020-03-04 14:44:41,084 cfg.model.bias_initializer         : zeros\n",
            "2020-03-04 14:44:41,084 cfg.model.init_gain                : 1.0\n",
            "2020-03-04 14:44:41,084 cfg.model.embed_initializer        : xavier\n",
            "2020-03-04 14:44:41,084 cfg.model.embed_init_gain          : 1.0\n",
            "2020-03-04 14:44:41,084 cfg.model.tied_embeddings          : True\n",
            "2020-03-04 14:44:41,084 cfg.model.tied_softmax             : True\n",
            "2020-03-04 14:44:41,084 cfg.model.encoder.type             : transformer\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.num_layers       : 6\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.num_heads        : 4\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.embeddings.scale : True\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.hidden_size      : 256\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.ff_size          : 1024\n",
            "2020-03-04 14:44:41,085 cfg.model.encoder.dropout          : 0.3\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.type             : transformer\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.num_layers       : 6\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.num_heads        : 4\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.embeddings.scale : True\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.hidden_size      : 256\n",
            "2020-03-04 14:44:41,086 cfg.model.decoder.ff_size          : 1024\n",
            "2020-03-04 14:44:41,087 cfg.model.decoder.dropout          : 0.3\n",
            "2020-03-04 14:44:41,087 Data set sizes: \n",
            "\ttrain 415749,\n",
            "\tvalid 1000,\n",
            "\ttest 2662\n",
            "2020-03-04 14:44:41,087 First training example:\n",
            "\t[SRC] TRAIN YOUR CHILDREN : “ I teach my children to check the expiration date of any packaged food items , such as snacks , before they buy them . ” ​ — Ruth , Nigeria\n",
            "\t[TRG] KỌ́ ÀWỌN ỌMỌ RẸ : “ Mo kọ́ àwọn ọmọ mi pé kí wọ́n tó ra oúnjẹ bí ìpápánu , tó wà nínú agolo , ike , bébà , tàbí ọ̀rá , kí wọ́n máa yẹ ara oúnjẹ náà wò kí wọ́n lè mọ déètì tó máa bà jẹ́ . ” — Ruth , Nàìjíríà\n",
            "2020-03-04 14:44:41,087 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) tó (8) to (9) of\n",
            "2020-03-04 14:44:41,088 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) tó (8) to (9) of\n",
            "2020-03-04 14:44:41,088 Number of Src words (types): 99533\n",
            "2020-03-04 14:44:41,088 Number of Trg words (types): 99533\n",
            "2020-03-04 14:44:41,088 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=99533),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=99533))\n",
            "2020-03-04 14:44:41,166 EPOCH 1\n",
            "2020-03-04 14:45:24,742 Epoch   1 Step:      100 Batch Loss:     5.564838 Tokens per Sec:     4497, Lr: 0.000300\n",
            "2020-03-04 14:46:07,870 Epoch   1 Step:      200 Batch Loss:     5.507414 Tokens per Sec:     4495, Lr: 0.000300\n",
            "2020-03-04 14:46:51,768 Epoch   1 Step:      300 Batch Loss:     5.403301 Tokens per Sec:     4478, Lr: 0.000300\n",
            "2020-03-04 14:47:36,700 Epoch   1 Step:      400 Batch Loss:     5.416591 Tokens per Sec:     4265, Lr: 0.000300\n",
            "2020-03-04 14:48:21,416 Epoch   1 Step:      500 Batch Loss:     5.295098 Tokens per Sec:     4361, Lr: 0.000300\n",
            "2020-03-04 14:49:07,005 Epoch   1 Step:      600 Batch Loss:     4.970981 Tokens per Sec:     4242, Lr: 0.000300\n",
            "2020-03-04 14:49:53,256 Epoch   1 Step:      700 Batch Loss:     4.908632 Tokens per Sec:     4246, Lr: 0.000300\n",
            "2020-03-04 14:50:38,920 Epoch   1 Step:      800 Batch Loss:     4.871610 Tokens per Sec:     4218, Lr: 0.000300\n",
            "2020-03-04 14:51:24,888 Epoch   1 Step:      900 Batch Loss:     4.742068 Tokens per Sec:     4173, Lr: 0.000300\n",
            "2020-03-04 14:52:10,818 Epoch   1 Step:     1000 Batch Loss:     4.754518 Tokens per Sec:     4161, Lr: 0.000300\n",
            "2020-03-04 14:54:21,145 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 14:54:21,145 Saving new checkpoint.\n",
            "2020-03-04 14:54:24,422 Example #0\n",
            "2020-03-04 14:54:24,423 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 14:54:24,423 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 14:54:24,423 \tHypothesis: Ó bá ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń\n",
            "2020-03-04 14:54:24,423 Example #1\n",
            "2020-03-04 14:54:24,424 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 14:54:24,424 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 14:54:24,424 \tHypothesis: Ó ṣe pé àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn èèyàn , àwọn èèyàn , àwọn àwọn àwọn àwọn àwọn àwọn àwọn wọ́n ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń\n",
            "2020-03-04 14:54:24,424 Example #2\n",
            "2020-03-04 14:54:24,425 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 14:54:24,425 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 14:54:24,425 \tHypothesis: Ó ṣe pé àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn àwọn\n",
            "2020-03-04 14:54:24,425 Example #3\n",
            "2020-03-04 14:54:24,426 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 14:54:24,426 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 14:54:24,426 \tHypothesis: Ó bá ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń ń\n",
            "2020-03-04 14:54:24,427 Validation result (greedy) at epoch   1, step     1000: bleu:   0.00, loss: 115148.3516, ppl: 102.7062, duration: 133.6076s\n",
            "2020-03-04 14:55:11,348 Epoch   1 Step:     1100 Batch Loss:     4.770444 Tokens per Sec:     4153, Lr: 0.000300\n",
            "2020-03-04 14:55:58,089 Epoch   1 Step:     1200 Batch Loss:     4.398784 Tokens per Sec:     4178, Lr: 0.000300\n",
            "2020-03-04 14:56:45,180 Epoch   1 Step:     1300 Batch Loss:     4.384159 Tokens per Sec:     4192, Lr: 0.000300\n",
            "2020-03-04 14:57:32,000 Epoch   1 Step:     1400 Batch Loss:     4.385372 Tokens per Sec:     4149, Lr: 0.000300\n",
            "2020-03-04 14:58:18,462 Epoch   1 Step:     1500 Batch Loss:     4.263248 Tokens per Sec:     4127, Lr: 0.000300\n",
            "2020-03-04 14:59:04,637 Epoch   1 Step:     1600 Batch Loss:     3.934861 Tokens per Sec:     4157, Lr: 0.000300\n",
            "2020-03-04 14:59:50,598 Epoch   1 Step:     1700 Batch Loss:     4.184441 Tokens per Sec:     4242, Lr: 0.000300\n",
            "2020-03-04 15:00:37,220 Epoch   1 Step:     1800 Batch Loss:     4.070102 Tokens per Sec:     4090, Lr: 0.000300\n",
            "2020-03-04 15:01:23,459 Epoch   1 Step:     1900 Batch Loss:     4.037801 Tokens per Sec:     4185, Lr: 0.000300\n",
            "2020-03-04 15:02:09,697 Epoch   1 Step:     2000 Batch Loss:     4.627933 Tokens per Sec:     4147, Lr: 0.000300\n",
            "2020-03-04 15:03:30,463 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 15:03:30,463 Saving new checkpoint.\n",
            "2020-03-04 15:03:33,672 Example #0\n",
            "2020-03-04 15:03:33,673 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 15:03:33,673 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 15:03:33,673 \tHypothesis: Ó ṣe pé òun ṣe pàtàkì pé òun ṣe ń ṣe àwọn èèyàn tó wà nínú Bíbélì .\n",
            "2020-03-04 15:03:33,674 Example #1\n",
            "2020-03-04 15:03:33,674 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 15:03:33,674 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 15:03:33,675 \tHypothesis: Ó sì máa ń ṣe ohun tó wà nínú àwọn èèyàn .\n",
            "2020-03-04 15:03:33,675 Example #2\n",
            "2020-03-04 15:03:33,676 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 15:03:33,676 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 15:03:33,676 \tHypothesis: Kí ni wọ́n ṣe ń ṣe pé òun ò ṣe ohun tó ń ṣe ?\n",
            "2020-03-04 15:03:33,676 Example #3\n",
            "2020-03-04 15:03:33,677 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 15:03:33,677 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 15:03:33,677 \tHypothesis: Ó sì máa ń ṣe pé àwọn èèyàn kò sì máa ń ṣe ohun tó ń ṣe .\n",
            "2020-03-04 15:03:33,677 Validation result (greedy) at epoch   1, step     2000: bleu:   2.11, loss: 97475.0938, ppl:  50.4489, duration: 83.9796s\n",
            "2020-03-04 15:04:20,492 Epoch   1 Step:     2100 Batch Loss:     4.027338 Tokens per Sec:     4224, Lr: 0.000300\n",
            "2020-03-04 15:05:06,758 Epoch   1 Step:     2200 Batch Loss:     3.645193 Tokens per Sec:     4146, Lr: 0.000300\n",
            "2020-03-04 15:05:53,627 Epoch   1 Step:     2300 Batch Loss:     3.899050 Tokens per Sec:     4213, Lr: 0.000300\n",
            "2020-03-04 15:06:40,053 Epoch   1 Step:     2400 Batch Loss:     3.672072 Tokens per Sec:     4098, Lr: 0.000300\n",
            "2020-03-04 15:07:26,490 Epoch   1 Step:     2500 Batch Loss:     3.692725 Tokens per Sec:     4228, Lr: 0.000300\n",
            "2020-03-04 15:08:13,280 Epoch   1 Step:     2600 Batch Loss:     3.727560 Tokens per Sec:     4043, Lr: 0.000300\n",
            "2020-03-04 15:08:59,479 Epoch   1 Step:     2700 Batch Loss:     3.852035 Tokens per Sec:     4235, Lr: 0.000300\n",
            "2020-03-04 15:09:45,939 Epoch   1 Step:     2800 Batch Loss:     3.641521 Tokens per Sec:     4214, Lr: 0.000300\n",
            "2020-03-04 15:10:32,319 Epoch   1 Step:     2900 Batch Loss:     3.564739 Tokens per Sec:     4201, Lr: 0.000300\n",
            "2020-03-04 15:11:18,775 Epoch   1 Step:     3000 Batch Loss:     3.540891 Tokens per Sec:     4143, Lr: 0.000300\n",
            "2020-03-04 15:13:04,835 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 15:13:04,836 Saving new checkpoint.\n",
            "2020-03-04 15:13:08,332 Example #0\n",
            "2020-03-04 15:13:08,334 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 15:13:08,334 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 15:13:08,335 \tHypothesis: Ó jẹ́ ká máa ṣe kedere pé ó ní láti ṣe ohun tí Jésù fi hàn pé ó jẹ́ ká máa ṣe ohun tí Jésù Kristi Kristi .\n",
            "2020-03-04 15:13:08,335 Example #1\n",
            "2020-03-04 15:13:08,335 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 15:13:08,336 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 15:13:08,336 \tHypothesis: Ní báyìí , mo ti ń ṣe iṣẹ́ ìwàásù .\n",
            "2020-03-04 15:13:08,336 Example #2\n",
            "2020-03-04 15:13:08,336 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 15:13:08,336 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 15:13:08,337 \tHypothesis: Ǹjẹ́ mo máa ń ṣe ohun tó ń ṣe pàtàkì jù lọ , kí n sì máa ń ṣe ohun tó ń ṣe ?\n",
            "2020-03-04 15:13:08,337 Example #3\n",
            "2020-03-04 15:13:08,337 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 15:13:08,337 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 15:13:08,338 \tHypothesis: Ó dájú pé ó ṣeé ṣe kó o máa ṣe ohun tó o bá fẹ́ , àmọ́ ó máa ń ṣe ohun tó o bá fẹ́ .\n",
            "2020-03-04 15:13:08,338 Validation result (greedy) at epoch   1, step     3000: bleu:   3.20, loss: 87086.5312, ppl:  33.2176, duration: 109.5623s\n",
            "2020-03-04 15:13:55,663 Epoch   1 Step:     3100 Batch Loss:     3.557585 Tokens per Sec:     4118, Lr: 0.000300\n",
            "2020-03-04 15:14:43,056 Epoch   1 Step:     3200 Batch Loss:     3.518675 Tokens per Sec:     4136, Lr: 0.000300\n",
            "2020-03-04 15:15:29,911 Epoch   1 Step:     3300 Batch Loss:     3.392982 Tokens per Sec:     4294, Lr: 0.000300\n",
            "2020-03-04 15:16:16,285 Epoch   1 Step:     3400 Batch Loss:     3.475251 Tokens per Sec:     4156, Lr: 0.000300\n",
            "2020-03-04 15:17:03,690 Epoch   1 Step:     3500 Batch Loss:     3.304292 Tokens per Sec:     4200, Lr: 0.000300\n",
            "2020-03-04 15:17:49,665 Epoch   1 Step:     3600 Batch Loss:     3.296428 Tokens per Sec:     4259, Lr: 0.000300\n",
            "2020-03-04 15:18:36,357 Epoch   1 Step:     3700 Batch Loss:     3.517574 Tokens per Sec:     4103, Lr: 0.000300\n",
            "2020-03-04 15:19:23,047 Epoch   1 Step:     3800 Batch Loss:     3.374401 Tokens per Sec:     4087, Lr: 0.000300\n",
            "2020-03-04 15:20:09,792 Epoch   1 Step:     3900 Batch Loss:     3.167128 Tokens per Sec:     4163, Lr: 0.000300\n",
            "2020-03-04 15:20:56,520 Epoch   1 Step:     4000 Batch Loss:     3.266424 Tokens per Sec:     4284, Lr: 0.000300\n",
            "2020-03-04 15:22:19,728 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 15:22:19,728 Saving new checkpoint.\n",
            "2020-03-04 15:22:23,198 Example #0\n",
            "2020-03-04 15:22:23,199 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 15:22:23,199 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 15:22:23,199 \tHypothesis: Ó sì mú kí gbogbo èèyàn máa fi hàn pé ó máa ń ṣe ohun tó tọ́ nínú ayé .\n",
            "2020-03-04 15:22:23,199 Example #1\n",
            "2020-03-04 15:22:23,200 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 15:22:23,200 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 15:22:23,200 \tHypothesis: Mo ti ń ṣe iṣẹ́ ìwàásù .\n",
            "2020-03-04 15:22:23,200 Example #2\n",
            "2020-03-04 15:22:23,201 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 15:22:23,201 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 15:22:23,201 \tHypothesis: Ṣé mo máa ń fi àwọn nǹkan tó ń ṣe iṣẹ́ òjíṣẹ́ mi , kí n sì máa ṣe iṣẹ́ òjíṣẹ́ mi ?\n",
            "2020-03-04 15:22:23,201 Example #3\n",
            "2020-03-04 15:22:23,203 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 15:22:23,203 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 15:22:23,203 \tHypothesis: Ó ti wá rí i pé o ò ní ní máa ronú nípa rẹ̀ , àmọ́ o ò ní máa bá a nìṣó ní àwọn nǹkan tó ń ṣe .\n",
            "2020-03-04 15:22:23,203 Validation result (greedy) at epoch   1, step     4000: bleu:   4.46, loss: 79800.7500, ppl:  24.7792, duration: 86.6827s\n",
            "2020-03-04 15:23:09,723 Epoch   1 Step:     4100 Batch Loss:     3.232888 Tokens per Sec:     4118, Lr: 0.000300\n",
            "2020-03-04 15:23:56,392 Epoch   1 Step:     4200 Batch Loss:     2.893069 Tokens per Sec:     4189, Lr: 0.000300\n",
            "2020-03-04 15:24:42,431 Epoch   1 Step:     4300 Batch Loss:     3.378380 Tokens per Sec:     4126, Lr: 0.000300\n",
            "2020-03-04 15:25:28,674 Epoch   1 Step:     4400 Batch Loss:     3.096040 Tokens per Sec:     4139, Lr: 0.000300\n",
            "2020-03-04 15:26:15,236 Epoch   1 Step:     4500 Batch Loss:     3.317276 Tokens per Sec:     4267, Lr: 0.000300\n",
            "2020-03-04 15:27:01,148 Epoch   1 Step:     4600 Batch Loss:     3.189778 Tokens per Sec:     4121, Lr: 0.000300\n",
            "2020-03-04 15:27:47,427 Epoch   1 Step:     4700 Batch Loss:     3.062336 Tokens per Sec:     4069, Lr: 0.000300\n",
            "2020-03-04 15:28:33,136 Epoch   1 Step:     4800 Batch Loss:     3.076293 Tokens per Sec:     4239, Lr: 0.000300\n",
            "2020-03-04 15:29:18,858 Epoch   1 Step:     4900 Batch Loss:     3.109844 Tokens per Sec:     4190, Lr: 0.000300\n",
            "2020-03-04 15:30:05,381 Epoch   1 Step:     5000 Batch Loss:     3.127495 Tokens per Sec:     4261, Lr: 0.000300\n",
            "2020-03-04 15:31:26,285 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 15:31:26,286 Saving new checkpoint.\n",
            "2020-03-04 15:31:29,999 Example #0\n",
            "2020-03-04 15:31:30,000 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 15:31:30,000 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 15:31:30,000 \tHypothesis: Ó ṣe kedere pé ó jẹ́ kí gbogbo ohun tó wà nínú àdúrà Kristi , ó sì fi hàn pé ó jẹ́ kí Jésù máa ṣe ohun tó wà nínú ọ̀rọ̀ náà .\n",
            "2020-03-04 15:31:30,000 Example #1\n",
            "2020-03-04 15:31:30,001 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 15:31:30,001 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 15:31:30,001 \tHypothesis: Mo ti ń ṣe iṣẹ́ tó wà nínú iṣẹ́ náà .\n",
            "2020-03-04 15:31:30,001 Example #2\n",
            "2020-03-04 15:31:30,002 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 15:31:30,002 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 15:31:30,002 \tHypothesis: Ṣé mo ti ń ṣe àwọn ohun tó ń ṣe tí mo ń ṣe , kí n sì máa ṣe àwọn èèyàn tí wọ́n ń ṣe ?\n",
            "2020-03-04 15:31:30,002 Example #3\n",
            "2020-03-04 15:31:30,003 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 15:31:30,003 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 15:31:30,003 \tHypothesis: Ó ti rí i pé ó máa ń ṣe ẹ́ , àmọ́ ó máa ń gbàdúrà , àmọ́ o máa ń gbàdúrà .\n",
            "2020-03-04 15:31:30,003 Validation result (greedy) at epoch   1, step     5000: bleu:   6.21, loss: 74459.4297, ppl:  19.9884, duration: 84.6218s\n",
            "2020-03-04 15:32:16,256 Epoch   1 Step:     5100 Batch Loss:     3.167685 Tokens per Sec:     4162, Lr: 0.000300\n",
            "2020-03-04 15:33:02,067 Epoch   1 Step:     5200 Batch Loss:     2.987945 Tokens per Sec:     4247, Lr: 0.000300\n",
            "2020-03-04 15:33:28,875 Epoch   1: total training loss 20898.62\n",
            "2020-03-04 15:33:28,876 EPOCH 2\n",
            "2020-03-04 15:33:48,797 Epoch   2 Step:     5300 Batch Loss:     2.989642 Tokens per Sec:     4056, Lr: 0.000300\n",
            "2020-03-04 15:34:35,489 Epoch   2 Step:     5400 Batch Loss:     3.174204 Tokens per Sec:     4197, Lr: 0.000300\n",
            "2020-03-04 15:35:21,337 Epoch   2 Step:     5500 Batch Loss:     3.058242 Tokens per Sec:     4188, Lr: 0.000300\n",
            "2020-03-04 15:36:07,056 Epoch   2 Step:     5600 Batch Loss:     3.017429 Tokens per Sec:     4247, Lr: 0.000300\n",
            "2020-03-04 15:36:53,448 Epoch   2 Step:     5700 Batch Loss:     3.146633 Tokens per Sec:     4283, Lr: 0.000300\n",
            "2020-03-04 15:37:39,429 Epoch   2 Step:     5800 Batch Loss:     2.943908 Tokens per Sec:     4260, Lr: 0.000300\n",
            "2020-03-04 15:38:26,197 Epoch   2 Step:     5900 Batch Loss:     2.871317 Tokens per Sec:     4143, Lr: 0.000300\n",
            "2020-03-04 15:39:12,325 Epoch   2 Step:     6000 Batch Loss:     3.119039 Tokens per Sec:     4215, Lr: 0.000300\n",
            "2020-03-04 15:40:24,934 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 15:40:24,935 Saving new checkpoint.\n",
            "2020-03-04 15:40:28,437 Example #0\n",
            "2020-03-04 15:40:28,438 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 15:40:28,438 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 15:40:28,439 \tHypothesis: Ó dá wa lójú pé ó ń fi ìfẹ́ hàn , ó sì ń fi hàn pé a ń fi ìfẹ́ hàn .\n",
            "2020-03-04 15:40:28,439 Example #1\n",
            "2020-03-04 15:40:28,439 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 15:40:28,439 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 15:40:28,439 \tHypothesis: Ní báyìí , mo ti rí i pé mo ti ń ṣiṣẹ́ kára .\n",
            "2020-03-04 15:40:28,440 Example #2\n",
            "2020-03-04 15:40:28,440 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 15:40:28,440 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 15:40:28,440 \tHypothesis: Ṣé mo máa ń rí àwọn nǹkan tí mo ń ṣe nípa tẹ̀mí , kí ni mo sì máa ń ṣe ?\n",
            "2020-03-04 15:40:28,440 Example #3\n",
            "2020-03-04 15:40:28,441 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 15:40:28,441 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 15:40:28,441 \tHypothesis: Ó ti ń ṣe é , ó sì ń mú kó o máa gbàdúrà , àmọ́ kó o máa gbàdúrà sí ọ .\n",
            "2020-03-04 15:40:28,442 Validation result (greedy) at epoch   2, step     6000: bleu:   7.75, loss: 70707.5234, ppl:  17.1883, duration: 76.1157s\n",
            "2020-03-04 15:41:14,168 Epoch   2 Step:     6100 Batch Loss:     2.820704 Tokens per Sec:     4115, Lr: 0.000300\n",
            "2020-03-04 15:42:00,180 Epoch   2 Step:     6200 Batch Loss:     2.958152 Tokens per Sec:     4159, Lr: 0.000300\n",
            "2020-03-04 15:42:46,583 Epoch   2 Step:     6300 Batch Loss:     2.812463 Tokens per Sec:     4228, Lr: 0.000300\n",
            "2020-03-04 15:43:33,355 Epoch   2 Step:     6400 Batch Loss:     2.817369 Tokens per Sec:     4250, Lr: 0.000300\n",
            "2020-03-04 15:44:18,693 Epoch   2 Step:     6500 Batch Loss:     3.313298 Tokens per Sec:     4224, Lr: 0.000300\n",
            "2020-03-04 15:45:04,880 Epoch   2 Step:     6600 Batch Loss:     2.892287 Tokens per Sec:     4174, Lr: 0.000300\n",
            "2020-03-04 15:45:51,585 Epoch   2 Step:     6700 Batch Loss:     4.105445 Tokens per Sec:     4130, Lr: 0.000300\n",
            "2020-03-04 15:46:36,964 Epoch   2 Step:     6800 Batch Loss:     2.832222 Tokens per Sec:     4277, Lr: 0.000300\n",
            "2020-03-04 15:47:23,300 Epoch   2 Step:     6900 Batch Loss:     2.601199 Tokens per Sec:     4235, Lr: 0.000300\n",
            "2020-03-04 15:48:10,070 Epoch   2 Step:     7000 Batch Loss:     2.792473 Tokens per Sec:     4185, Lr: 0.000300\n",
            "2020-03-04 15:49:51,937 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 15:49:51,937 Saving new checkpoint.\n",
            "2020-03-04 15:49:55,748 Example #0\n",
            "2020-03-04 15:49:55,749 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 15:49:55,749 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 15:49:55,749 \tHypothesis: Ó jẹ́ ká máa fi gbogbo ìgbésí ayé rẹ̀ hàn , ó sì ń fi ẹ̀bùn rere àìlẹ́tọ̀ọ́sí Kristi hàn .\n",
            "2020-03-04 15:49:55,749 Example #1\n",
            "2020-03-04 15:49:55,749 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 15:49:55,749 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 15:49:55,749 \tHypothesis: Mo ti rí i pé mo ti ń ṣe iṣẹ́ tó ń ṣe .\n",
            "2020-03-04 15:49:55,749 Example #2\n",
            "2020-03-04 15:49:55,750 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 15:49:55,750 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 15:49:55,750 \tHypothesis: Ṣé mo máa ń rí àwọn nǹkan tó ń mú kí àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn mi máa ń ṣe ?\n",
            "2020-03-04 15:49:55,750 Example #3\n",
            "2020-03-04 15:49:55,750 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 15:49:55,750 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 15:49:55,750 \tHypothesis: Ó ti rí i pé ó ti ń ṣe ẹ́ láǹfààní , àmọ́ ó máa ń mú kó o máa gbàdúrà .\n",
            "2020-03-04 15:49:55,751 Validation result (greedy) at epoch   2, step     7000: bleu:   9.56, loss: 66718.9375, ppl:  14.6404, duration: 105.6800s\n",
            "2020-03-04 15:50:41,136 Epoch   2 Step:     7100 Batch Loss:     2.725598 Tokens per Sec:     4194, Lr: 0.000300\n",
            "2020-03-04 15:51:27,517 Epoch   2 Step:     7200 Batch Loss:     2.886862 Tokens per Sec:     4274, Lr: 0.000300\n",
            "2020-03-04 15:52:13,883 Epoch   2 Step:     7300 Batch Loss:     2.957735 Tokens per Sec:     4153, Lr: 0.000300\n",
            "2020-03-04 15:53:00,031 Epoch   2 Step:     7400 Batch Loss:     2.819317 Tokens per Sec:     4167, Lr: 0.000300\n",
            "2020-03-04 15:53:45,897 Epoch   2 Step:     7500 Batch Loss:     2.680435 Tokens per Sec:     4197, Lr: 0.000300\n",
            "2020-03-04 15:54:32,084 Epoch   2 Step:     7600 Batch Loss:     2.772745 Tokens per Sec:     4198, Lr: 0.000300\n",
            "2020-03-04 15:55:17,427 Epoch   2 Step:     7700 Batch Loss:     2.655432 Tokens per Sec:     4113, Lr: 0.000300\n",
            "2020-03-04 15:56:03,340 Epoch   2 Step:     7800 Batch Loss:     2.604064 Tokens per Sec:     4242, Lr: 0.000300\n",
            "2020-03-04 15:56:49,423 Epoch   2 Step:     7900 Batch Loss:     2.541819 Tokens per Sec:     4209, Lr: 0.000300\n",
            "2020-03-04 15:57:36,031 Epoch   2 Step:     8000 Batch Loss:     2.544629 Tokens per Sec:     4213, Lr: 0.000300\n",
            "2020-03-04 15:58:49,556 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 15:58:49,556 Saving new checkpoint.\n",
            "2020-03-04 15:58:53,191 Example #0\n",
            "2020-03-04 15:58:53,192 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 15:58:53,192 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 15:58:53,192 \tHypothesis: Ó jẹ́ Orísun ìyè , ẹni tí ó jẹ́ ẹ̀bùn rere fún Kristi .\n",
            "2020-03-04 15:58:53,192 Example #1\n",
            "2020-03-04 15:58:53,193 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 15:58:53,193 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 15:58:53,193 \tHypothesis: Ní báyìí , mo ti rí i pé iṣẹ́ tí mò ń ṣe yìí .\n",
            "2020-03-04 15:58:53,193 Example #2\n",
            "2020-03-04 15:58:53,194 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 15:58:53,194 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 15:58:53,194 \tHypothesis: Ṣé mo máa ń ṣe àwọn nǹkan tó ń ṣe mí ju ti àwọn èèyàn lọ , kí n sì máa ṣe ?\n",
            "2020-03-04 15:58:53,194 Example #3\n",
            "2020-03-04 15:58:53,194 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 15:58:53,194 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 15:58:53,195 \tHypothesis: Ó ti ń ṣe ẹ́ láǹfààní , ó sì ń fi sùúrù ṣe é , àmọ́ ó ń fi sùúrù ṣe é .\n",
            "2020-03-04 15:58:53,195 Validation result (greedy) at epoch   2, step     8000: bleu:  11.41, loss: 63464.6289, ppl:  12.8441, duration: 77.1637s\n",
            "2020-03-04 15:59:38,437 Epoch   2 Step:     8100 Batch Loss:     2.993184 Tokens per Sec:     4174, Lr: 0.000300\n",
            "2020-03-04 16:00:24,847 Epoch   2 Step:     8200 Batch Loss:     3.000679 Tokens per Sec:     4147, Lr: 0.000300\n",
            "2020-03-04 16:01:11,655 Epoch   2 Step:     8300 Batch Loss:     2.759903 Tokens per Sec:     4152, Lr: 0.000300\n",
            "2020-03-04 16:01:58,173 Epoch   2 Step:     8400 Batch Loss:     2.491884 Tokens per Sec:     4194, Lr: 0.000300\n",
            "2020-03-04 16:02:44,867 Epoch   2 Step:     8500 Batch Loss:     2.694126 Tokens per Sec:     4266, Lr: 0.000300\n",
            "2020-03-04 16:03:30,934 Epoch   2 Step:     8600 Batch Loss:     2.441365 Tokens per Sec:     4116, Lr: 0.000300\n",
            "2020-03-04 16:04:16,642 Epoch   2 Step:     8700 Batch Loss:     2.436101 Tokens per Sec:     4261, Lr: 0.000300\n",
            "2020-03-04 16:05:02,787 Epoch   2 Step:     8800 Batch Loss:     2.699770 Tokens per Sec:     4270, Lr: 0.000300\n",
            "2020-03-04 16:05:49,539 Epoch   2 Step:     8900 Batch Loss:     2.602569 Tokens per Sec:     4122, Lr: 0.000300\n",
            "2020-03-04 16:06:35,713 Epoch   2 Step:     9000 Batch Loss:     2.702884 Tokens per Sec:     4160, Lr: 0.000300\n",
            "2020-03-04 16:07:52,859 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 16:07:52,859 Saving new checkpoint.\n",
            "2020-03-04 16:07:56,112 Example #0\n",
            "2020-03-04 16:07:56,113 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 16:07:56,113 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 16:07:56,114 \tHypothesis: Ẹni tí ń bẹ nínú ìgbésí ayé , Ẹni tí ń fúnni ní ẹ̀bùn rere nípasẹ̀ Kristi .\n",
            "2020-03-04 16:07:56,114 Example #1\n",
            "2020-03-04 16:07:56,114 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 16:07:56,114 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 16:07:56,114 \tHypothesis: Nísinsìnyí , mo ti wá rí i pé iṣẹ́ tí wọ́n ń ṣe fún un .\n",
            "2020-03-04 16:07:56,115 Example #2\n",
            "2020-03-04 16:07:56,115 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 16:07:56,115 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 16:07:56,115 \tHypothesis: Ǹjẹ́ mo máa ń ṣe àwọn ohun tó ń ṣe pàtàkì ju àjọṣe tí Jèhófà ní pẹ̀lú àwọn èèyàn mi lọ ?\n",
            "2020-03-04 16:07:56,115 Example #3\n",
            "2020-03-04 16:07:56,115 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 16:07:56,116 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 16:07:56,116 \tHypothesis: Ó ti wá ṣe kedere pé , ó sì ń mú kí inú rẹ dùn , àmọ́ ó ń múra tán láti máa fi sùúrù sin Jèhófà .\n",
            "2020-03-04 16:07:56,116 Validation result (greedy) at epoch   2, step     9000: bleu:  12.01, loss: 62754.4844, ppl:  12.4823, duration: 80.4019s\n",
            "2020-03-04 16:08:42,856 Epoch   2 Step:     9100 Batch Loss:     2.599545 Tokens per Sec:     4190, Lr: 0.000300\n",
            "2020-03-04 16:09:28,799 Epoch   2 Step:     9200 Batch Loss:     2.741610 Tokens per Sec:     4282, Lr: 0.000300\n",
            "2020-03-04 16:10:15,472 Epoch   2 Step:     9300 Batch Loss:     2.787201 Tokens per Sec:     4246, Lr: 0.000300\n",
            "2020-03-04 16:11:01,897 Epoch   2 Step:     9400 Batch Loss:     2.511764 Tokens per Sec:     4124, Lr: 0.000300\n",
            "2020-03-04 16:11:48,382 Epoch   2 Step:     9500 Batch Loss:     2.477360 Tokens per Sec:     4170, Lr: 0.000300\n",
            "2020-03-04 16:12:34,334 Epoch   2 Step:     9600 Batch Loss:     2.324592 Tokens per Sec:     4214, Lr: 0.000300\n",
            "2020-03-04 16:13:20,233 Epoch   2 Step:     9700 Batch Loss:     2.492966 Tokens per Sec:     4217, Lr: 0.000300\n",
            "2020-03-04 16:14:06,418 Epoch   2 Step:     9800 Batch Loss:     2.614703 Tokens per Sec:     4322, Lr: 0.000300\n",
            "2020-03-04 16:14:52,971 Epoch   2 Step:     9900 Batch Loss:     2.557074 Tokens per Sec:     4186, Lr: 0.000300\n",
            "2020-03-04 16:15:39,849 Epoch   2 Step:    10000 Batch Loss:     2.557532 Tokens per Sec:     4257, Lr: 0.000300\n",
            "2020-03-04 16:16:47,198 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 16:16:47,198 Saving new checkpoint.\n",
            "2020-03-04 16:16:51,041 Example #0\n",
            "2020-03-04 16:16:51,042 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 16:16:51,042 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 16:16:51,043 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tí ó jẹ́ ẹ̀bùn pípéye nípa Kristi .\n",
            "2020-03-04 16:16:51,043 Example #1\n",
            "2020-03-04 16:16:51,043 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 16:16:51,043 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 16:16:51,043 \tHypothesis: Mo ti wá rí i pé mo fẹ́ ṣe iṣẹ́ tó yẹ kí n ṣe .\n",
            "2020-03-04 16:16:51,043 Example #2\n",
            "2020-03-04 16:16:51,044 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 16:16:51,044 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 16:16:51,044 \tHypothesis: Ǹjẹ́ mo máa ń ṣe ohun tó dára ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-03-04 16:16:51,044 Example #3\n",
            "2020-03-04 16:16:51,044 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 16:16:51,044 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 16:16:51,044 \tHypothesis: Ó ti ń ṣe ẹ́ láǹfààní , ó sì ń fi sùúrù ṣe ẹ́ , àmọ́ ó ń rìn .\n",
            "2020-03-04 16:16:51,044 Validation result (greedy) at epoch   2, step    10000: bleu:  15.05, loss: 58180.7188, ppl:  10.3847, duration: 71.1946s\n",
            "2020-03-04 16:17:37,092 Epoch   2 Step:    10100 Batch Loss:     2.512979 Tokens per Sec:     4178, Lr: 0.000300\n",
            "2020-03-04 16:18:23,554 Epoch   2 Step:    10200 Batch Loss:     2.484155 Tokens per Sec:     4286, Lr: 0.000300\n",
            "2020-03-04 16:19:10,051 Epoch   2 Step:    10300 Batch Loss:     2.393490 Tokens per Sec:     4124, Lr: 0.000300\n",
            "2020-03-04 16:19:56,802 Epoch   2 Step:    10400 Batch Loss:     2.587463 Tokens per Sec:     4309, Lr: 0.000300\n",
            "2020-03-04 16:20:42,668 Epoch   2 Step:    10500 Batch Loss:     2.633358 Tokens per Sec:     4203, Lr: 0.000300\n",
            "2020-03-04 16:20:45,899 Epoch   2: total training loss 14375.16\n",
            "2020-03-04 16:20:45,899 EPOCH 3\n",
            "2020-03-04 16:21:29,366 Epoch   3 Step:    10600 Batch Loss:     2.340113 Tokens per Sec:     4284, Lr: 0.000300\n",
            "2020-03-04 16:22:15,715 Epoch   3 Step:    10700 Batch Loss:     2.502595 Tokens per Sec:     4263, Lr: 0.000300\n",
            "2020-03-04 16:23:02,319 Epoch   3 Step:    10800 Batch Loss:     2.426071 Tokens per Sec:     4182, Lr: 0.000300\n",
            "2020-03-04 16:23:48,129 Epoch   3 Step:    10900 Batch Loss:     2.374894 Tokens per Sec:     4145, Lr: 0.000300\n",
            "2020-03-04 16:24:34,320 Epoch   3 Step:    11000 Batch Loss:     2.286048 Tokens per Sec:     4186, Lr: 0.000300\n",
            "2020-03-04 16:25:27,820 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 16:25:27,820 Saving new checkpoint.\n",
            "2020-03-04 16:25:31,292 Example #0\n",
            "2020-03-04 16:25:31,293 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 16:25:31,293 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 16:25:31,294 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn pípéye nípa Kristi .\n",
            "2020-03-04 16:25:31,294 Example #1\n",
            "2020-03-04 16:25:31,294 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 16:25:31,294 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 16:25:31,295 \tHypothesis: Mo ti rí i pé ó yẹ kí n máa ṣe iṣẹ́ náà .\n",
            "2020-03-04 16:25:31,295 Example #2\n",
            "2020-03-04 16:25:31,295 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 16:25:31,296 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 16:25:31,296 \tHypothesis: Ṣé mo máa ń mọ àwọn nǹkan tó pọ̀ jù lọ tí mo bá ń ṣe pẹ̀lú Jèhófà àti àwọn èèyàn ?\n",
            "2020-03-04 16:25:31,296 Example #3\n",
            "2020-03-04 16:25:31,296 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 16:25:31,297 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 16:25:31,297 \tHypothesis: Ó ti wá rí i pé ó ń jẹ́ kó o túbọ̀ ní sùúrù , àmọ́ ó ń fi sùúrù rìn .\n",
            "2020-03-04 16:25:31,297 Validation result (greedy) at epoch   3, step    11000: bleu:  16.10, loss: 56614.9805, ppl:   9.7508, duration: 56.9762s\n",
            "2020-03-04 16:26:17,824 Epoch   3 Step:    11100 Batch Loss:     2.542009 Tokens per Sec:     4221, Lr: 0.000300\n",
            "2020-03-04 16:27:03,776 Epoch   3 Step:    11200 Batch Loss:     2.357614 Tokens per Sec:     4265, Lr: 0.000300\n",
            "2020-03-04 16:27:49,693 Epoch   3 Step:    11300 Batch Loss:     2.580353 Tokens per Sec:     4229, Lr: 0.000300\n",
            "2020-03-04 16:28:34,576 Epoch   3 Step:    11400 Batch Loss:     2.519833 Tokens per Sec:     4142, Lr: 0.000300\n",
            "2020-03-04 16:29:21,100 Epoch   3 Step:    11500 Batch Loss:     2.424560 Tokens per Sec:     4236, Lr: 0.000300\n",
            "2020-03-04 16:30:07,393 Epoch   3 Step:    11600 Batch Loss:     2.175067 Tokens per Sec:     4205, Lr: 0.000300\n",
            "2020-03-04 16:30:54,159 Epoch   3 Step:    11700 Batch Loss:     2.379701 Tokens per Sec:     4191, Lr: 0.000300\n",
            "2020-03-04 16:31:41,143 Epoch   3 Step:    11800 Batch Loss:     2.295418 Tokens per Sec:     4154, Lr: 0.000300\n",
            "2020-03-04 16:32:27,191 Epoch   3 Step:    11900 Batch Loss:     2.304708 Tokens per Sec:     4176, Lr: 0.000300\n",
            "2020-03-04 16:33:13,137 Epoch   3 Step:    12000 Batch Loss:     2.253888 Tokens per Sec:     4233, Lr: 0.000300\n",
            "2020-03-04 16:34:09,757 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 16:34:09,758 Saving new checkpoint.\n",
            "2020-03-04 16:34:13,023 Example #0\n",
            "2020-03-04 16:34:13,024 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 16:34:13,024 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 16:34:13,024 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn rere nípasẹ̀ Kristi .\n",
            "2020-03-04 16:34:13,024 Example #1\n",
            "2020-03-04 16:34:13,025 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 16:34:13,025 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 16:34:13,025 \tHypothesis: Ní báyìí , mo ti rí iṣẹ́ tó yẹ kí n ṣe .\n",
            "2020-03-04 16:34:13,025 Example #2\n",
            "2020-03-04 16:34:13,025 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 16:34:13,025 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 16:34:13,025 \tHypothesis: Ṣé mo mọyì àwọn nǹkan ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn mi lọ ?\n",
            "2020-03-04 16:34:13,025 Example #3\n",
            "2020-03-04 16:34:13,025 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 16:34:13,026 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 16:34:13,026 \tHypothesis: Ó ti rí i pé ó ń mú kí ara rẹ balẹ̀ , àmọ́ ó ń fi sùúrù rìn .\n",
            "2020-03-04 16:34:13,026 Validation result (greedy) at epoch   3, step    12000: bleu:  17.56, loss: 55189.1875, ppl:   9.2073, duration: 59.8884s\n",
            "2020-03-04 16:34:58,720 Epoch   3 Step:    12100 Batch Loss:     2.194941 Tokens per Sec:     4318, Lr: 0.000300\n",
            "2020-03-04 16:35:44,657 Epoch   3 Step:    12200 Batch Loss:     2.306963 Tokens per Sec:     4181, Lr: 0.000300\n",
            "2020-03-04 16:36:30,196 Epoch   3 Step:    12300 Batch Loss:     2.436317 Tokens per Sec:     4271, Lr: 0.000300\n",
            "2020-03-04 16:37:15,902 Epoch   3 Step:    12400 Batch Loss:     2.457044 Tokens per Sec:     4209, Lr: 0.000300\n",
            "2020-03-04 16:38:01,906 Epoch   3 Step:    12500 Batch Loss:     2.133121 Tokens per Sec:     4234, Lr: 0.000300\n",
            "2020-03-04 16:38:48,268 Epoch   3 Step:    12600 Batch Loss:     2.389368 Tokens per Sec:     4169, Lr: 0.000300\n",
            "2020-03-04 16:39:34,685 Epoch   3 Step:    12700 Batch Loss:     2.549899 Tokens per Sec:     4206, Lr: 0.000300\n",
            "2020-03-04 16:40:19,262 Epoch   3 Step:    12800 Batch Loss:     2.352049 Tokens per Sec:     4235, Lr: 0.000300\n",
            "2020-03-04 16:41:05,634 Epoch   3 Step:    12900 Batch Loss:     2.231695 Tokens per Sec:     4251, Lr: 0.000300\n",
            "2020-03-04 16:41:51,871 Epoch   3 Step:    13000 Batch Loss:     2.222590 Tokens per Sec:     4190, Lr: 0.000300\n",
            "2020-03-04 16:42:51,467 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 16:42:51,468 Saving new checkpoint.\n",
            "2020-03-04 16:42:55,358 Example #0\n",
            "2020-03-04 16:42:55,360 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 16:42:55,360 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 16:42:55,360 \tHypothesis: Ẹni tó jẹ́ Orísun ìyè àìnípẹ̀kun ni Ẹni tó ń fúnni ní ẹ̀bùn pípéye nípasẹ̀ Kristi .\n",
            "2020-03-04 16:42:55,360 Example #1\n",
            "2020-03-04 16:42:55,360 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 16:42:55,360 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 16:42:55,361 \tHypothesis: Ní báyìí , mo ti rí i pé ó yẹ kí n máa ṣe iṣẹ́ .\n",
            "2020-03-04 16:42:55,361 Example #2\n",
            "2020-03-04 16:42:55,361 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 16:42:55,361 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 16:42:55,361 \tHypothesis: Ṣé mo máa ń ṣe àwọn nǹkan tó pọ̀ ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 16:42:55,361 Example #3\n",
            "2020-03-04 16:42:55,362 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 16:42:55,362 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 16:42:55,362 \tHypothesis: Ó ti pẹ́ tó fi máa ń rí i pé ó ń mú kó o máa ṣe nǹkan , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 16:42:55,362 Validation result (greedy) at epoch   3, step    13000: bleu:  18.59, loss: 53802.5625, ppl:   8.7078, duration: 63.4909s\n",
            "2020-03-04 16:43:41,310 Epoch   3 Step:    13100 Batch Loss:     2.437532 Tokens per Sec:     4234, Lr: 0.000300\n",
            "2020-03-04 16:44:27,839 Epoch   3 Step:    13200 Batch Loss:     2.206838 Tokens per Sec:     4238, Lr: 0.000300\n",
            "2020-03-04 16:45:13,393 Epoch   3 Step:    13300 Batch Loss:     2.396511 Tokens per Sec:     4282, Lr: 0.000300\n",
            "2020-03-04 16:45:59,482 Epoch   3 Step:    13400 Batch Loss:     2.386945 Tokens per Sec:     4167, Lr: 0.000300\n",
            "2020-03-04 16:46:46,390 Epoch   3 Step:    13500 Batch Loss:     2.300609 Tokens per Sec:     4187, Lr: 0.000300\n",
            "2020-03-04 16:47:32,593 Epoch   3 Step:    13600 Batch Loss:     2.339419 Tokens per Sec:     4195, Lr: 0.000300\n",
            "2020-03-04 16:48:18,255 Epoch   3 Step:    13700 Batch Loss:     2.118138 Tokens per Sec:     4066, Lr: 0.000300\n",
            "2020-03-04 16:49:04,623 Epoch   3 Step:    13800 Batch Loss:     2.237118 Tokens per Sec:     4198, Lr: 0.000300\n",
            "2020-03-04 16:49:50,928 Epoch   3 Step:    13900 Batch Loss:     2.274264 Tokens per Sec:     4214, Lr: 0.000300\n",
            "2020-03-04 16:50:37,562 Epoch   3 Step:    14000 Batch Loss:     2.445921 Tokens per Sec:     4180, Lr: 0.000300\n",
            "2020-03-04 16:51:34,540 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 16:51:34,540 Saving new checkpoint.\n",
            "2020-03-04 16:51:38,171 Example #0\n",
            "2020-03-04 16:51:38,172 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 16:51:38,172 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 16:51:38,172 \tHypothesis: Ẹni tó ń fúnni ní ẹ̀bùn ìyè , tó ń fúnni ní ẹ̀bùn ìyè àìnípẹ̀kun nípasẹ̀ Kristi .\n",
            "2020-03-04 16:51:38,172 Example #1\n",
            "2020-03-04 16:51:38,173 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 16:51:38,173 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 16:51:38,173 \tHypothesis: Ní báyìí , mo ti wá àyè kan láti ṣe iṣẹ́ .\n",
            "2020-03-04 16:51:38,173 Example #2\n",
            "2020-03-04 16:51:38,174 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 16:51:38,174 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 16:51:38,174 \tHypothesis: Ṣé mo máa ń mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà lọ , kí n sì máa bá àwọn èèyàn sọ̀rọ̀ ?\n",
            "2020-03-04 16:51:38,174 Example #3\n",
            "2020-03-04 16:51:38,175 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 16:51:38,175 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 16:51:38,175 \tHypothesis: Ó ti ń dàgbà sí i , ó sì ń mú kó o máa ṣe nǹkan , àmọ́ ó máa ń fi sùúrù rìn gbéregbère kiri .\n",
            "2020-03-04 16:51:38,176 Validation result (greedy) at epoch   3, step    14000: bleu:  19.37, loss: 52571.8516, ppl:   8.2872, duration: 60.6130s\n",
            "2020-03-04 16:52:23,728 Epoch   3 Step:    14100 Batch Loss:     2.138349 Tokens per Sec:     4218, Lr: 0.000300\n",
            "2020-03-04 16:53:09,132 Epoch   3 Step:    14200 Batch Loss:     2.299966 Tokens per Sec:     4223, Lr: 0.000300\n",
            "2020-03-04 16:53:54,630 Epoch   3 Step:    14300 Batch Loss:     2.181941 Tokens per Sec:     4250, Lr: 0.000300\n",
            "2020-03-04 16:54:41,008 Epoch   3 Step:    14400 Batch Loss:     2.173281 Tokens per Sec:     4223, Lr: 0.000300\n",
            "2020-03-04 16:55:26,463 Epoch   3 Step:    14500 Batch Loss:     2.222538 Tokens per Sec:     4210, Lr: 0.000300\n",
            "2020-03-04 16:56:12,474 Epoch   3 Step:    14600 Batch Loss:     2.052358 Tokens per Sec:     4194, Lr: 0.000300\n",
            "2020-03-04 16:56:58,540 Epoch   3 Step:    14700 Batch Loss:     2.286824 Tokens per Sec:     4181, Lr: 0.000300\n",
            "2020-03-04 16:57:44,891 Epoch   3 Step:    14800 Batch Loss:     2.614124 Tokens per Sec:     4291, Lr: 0.000300\n",
            "2020-03-04 16:58:30,668 Epoch   3 Step:    14900 Batch Loss:     2.284805 Tokens per Sec:     4167, Lr: 0.000300\n",
            "2020-03-04 16:59:18,366 Epoch   3 Step:    15000 Batch Loss:     2.120560 Tokens per Sec:     4112, Lr: 0.000300\n",
            "2020-03-04 17:00:11,576 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 17:00:11,576 Saving new checkpoint.\n",
            "2020-03-04 17:00:14,926 Example #0\n",
            "2020-03-04 17:00:14,927 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 17:00:14,927 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 17:00:14,928 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn ìràpadà nípasẹ̀ Kristi .\n",
            "2020-03-04 17:00:14,928 Example #1\n",
            "2020-03-04 17:00:14,928 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 17:00:14,928 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 17:00:14,928 \tHypothesis: Nísinsìnyí , mo ti rí iṣẹ́ tó dára jù lọ .\n",
            "2020-03-04 17:00:14,928 Example #2\n",
            "2020-03-04 17:00:14,928 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 17:00:14,929 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:00:14,929 \tHypothesis: Ǹjẹ́ mo máa ń ṣe àwọn nǹkan ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 17:00:14,929 Example #3\n",
            "2020-03-04 17:00:14,929 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 17:00:14,929 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 17:00:14,929 \tHypothesis: Ó ti ní ìrírí púpọ̀ sí i , ó sì ń mú kó o túbọ̀ máa ṣe ohun tó o bá ń ṣe , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-03-04 17:00:14,929 Validation result (greedy) at epoch   3, step    15000: bleu:  19.83, loss: 51497.7891, ppl:   7.9368, duration: 56.5630s\n",
            "2020-03-04 17:01:04,171 Epoch   3 Step:    15100 Batch Loss:     2.515746 Tokens per Sec:     3948, Lr: 0.000300\n",
            "2020-03-04 17:01:53,248 Epoch   3 Step:    15200 Batch Loss:     2.266107 Tokens per Sec:     4013, Lr: 0.000300\n",
            "2020-03-04 17:02:43,584 Epoch   3 Step:    15300 Batch Loss:     2.083863 Tokens per Sec:     3846, Lr: 0.000300\n",
            "2020-03-04 17:03:32,890 Epoch   3 Step:    15400 Batch Loss:     2.245037 Tokens per Sec:     3938, Lr: 0.000300\n",
            "2020-03-04 17:04:21,927 Epoch   3 Step:    15500 Batch Loss:     2.320621 Tokens per Sec:     4059, Lr: 0.000300\n",
            "2020-03-04 17:05:11,400 Epoch   3 Step:    15600 Batch Loss:     2.313440 Tokens per Sec:     4004, Lr: 0.000300\n",
            "2020-03-04 17:06:01,522 Epoch   3 Step:    15700 Batch Loss:     2.251445 Tokens per Sec:     3952, Lr: 0.000300\n",
            "2020-03-04 17:06:29,171 Epoch   3: total training loss 12142.88\n",
            "2020-03-04 17:06:29,172 EPOCH 4\n",
            "2020-03-04 17:06:51,476 Epoch   4 Step:    15800 Batch Loss:     2.118115 Tokens per Sec:     3818, Lr: 0.000300\n",
            "2020-03-04 17:07:40,068 Epoch   4 Step:    15900 Batch Loss:     2.242357 Tokens per Sec:     3972, Lr: 0.000300\n",
            "2020-03-04 17:08:29,476 Epoch   4 Step:    16000 Batch Loss:     2.106742 Tokens per Sec:     3972, Lr: 0.000300\n",
            "2020-03-04 17:09:31,137 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 17:09:31,137 Saving new checkpoint.\n",
            "2020-03-04 17:09:34,628 Example #0\n",
            "2020-03-04 17:09:34,629 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 17:09:34,629 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 17:09:34,629 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ń fúnni ní ẹ̀bùn nípasẹ̀ Kristi .\n",
            "2020-03-04 17:09:34,629 Example #1\n",
            "2020-03-04 17:09:34,630 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 17:09:34,630 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 17:09:34,630 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó yẹ kí n ṣe .\n",
            "2020-03-04 17:09:34,630 Example #2\n",
            "2020-03-04 17:09:34,631 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 17:09:34,631 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:09:34,631 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:09:34,631 Example #3\n",
            "2020-03-04 17:09:34,632 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 17:09:34,632 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 17:09:34,632 \tHypothesis: Ó ti ní ìrírí tó pọ̀ jù , ó sì ń sa gbogbo ipá rẹ láti ṣe é , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-03-04 17:09:34,632 Validation result (greedy) at epoch   4, step    16000: bleu:  20.55, loss: 50615.4219, ppl:   7.6601, duration: 65.1557s\n",
            "2020-03-04 17:10:24,686 Epoch   4 Step:    16100 Batch Loss:     2.257795 Tokens per Sec:     3902, Lr: 0.000300\n",
            "2020-03-04 17:11:13,393 Epoch   4 Step:    16200 Batch Loss:     2.071491 Tokens per Sec:     4020, Lr: 0.000300\n",
            "2020-03-04 17:12:02,598 Epoch   4 Step:    16300 Batch Loss:     2.162471 Tokens per Sec:     3875, Lr: 0.000300\n",
            "2020-03-04 17:12:52,076 Epoch   4 Step:    16400 Batch Loss:     2.134807 Tokens per Sec:     4003, Lr: 0.000300\n",
            "2020-03-04 17:13:41,810 Epoch   4 Step:    16500 Batch Loss:     2.173051 Tokens per Sec:     3866, Lr: 0.000300\n",
            "2020-03-04 17:14:30,185 Epoch   4 Step:    16600 Batch Loss:     2.283245 Tokens per Sec:     3992, Lr: 0.000300\n",
            "2020-03-04 17:15:19,660 Epoch   4 Step:    16700 Batch Loss:     2.151519 Tokens per Sec:     3907, Lr: 0.000300\n",
            "2020-03-04 17:16:08,806 Epoch   4 Step:    16800 Batch Loss:     2.094104 Tokens per Sec:     3979, Lr: 0.000300\n",
            "2020-03-04 17:16:58,064 Epoch   4 Step:    16900 Batch Loss:     2.160588 Tokens per Sec:     3855, Lr: 0.000300\n",
            "2020-03-04 17:17:47,238 Epoch   4 Step:    17000 Batch Loss:     2.093287 Tokens per Sec:     3899, Lr: 0.000300\n",
            "2020-03-04 17:19:22,390 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 17:19:22,390 Saving new checkpoint.\n",
            "2020-03-04 17:19:26,154 Example #0\n",
            "2020-03-04 17:19:26,155 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 17:19:26,155 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 17:19:26,155 \tHypothesis: Ẹni tó ń gbé ìgbésí ayé rẹ̀ ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn nípasẹ̀ Kristi .\n",
            "2020-03-04 17:19:26,155 Example #1\n",
            "2020-03-04 17:19:26,156 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 17:19:26,156 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 17:19:26,156 \tHypothesis: Mo wá rí i pé iṣẹ́ tó bófin mu ni mo ṣe .\n",
            "2020-03-04 17:19:26,156 Example #2\n",
            "2020-03-04 17:19:26,157 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 17:19:26,157 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:19:26,157 \tHypothesis: Ṣé mo máa ń mọ àwọn nǹkan ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 17:19:26,157 Example #3\n",
            "2020-03-04 17:19:26,157 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 17:19:26,158 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 17:19:26,158 \tHypothesis: Ó ti ní ìrírí tó pọ̀ jù , ó sì ń gbára dì ju bó ṣe ń ṣe ẹ́ lọ , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-03-04 17:19:26,158 Validation result (greedy) at epoch   4, step    17000: bleu:  20.27, loss: 50131.0352, ppl:   7.5122, duration: 98.9195s\n",
            "2020-03-04 17:20:15,680 Epoch   4 Step:    17100 Batch Loss:     2.422498 Tokens per Sec:     3929, Lr: 0.000300\n",
            "2020-03-04 17:21:03,785 Epoch   4 Step:    17200 Batch Loss:     2.195351 Tokens per Sec:     3989, Lr: 0.000300\n",
            "2020-03-04 17:21:53,419 Epoch   4 Step:    17300 Batch Loss:     2.076087 Tokens per Sec:     3941, Lr: 0.000300\n",
            "2020-03-04 17:22:43,558 Epoch   4 Step:    17400 Batch Loss:     2.129549 Tokens per Sec:     3847, Lr: 0.000300\n",
            "2020-03-04 17:23:32,909 Epoch   4 Step:    17500 Batch Loss:     2.098894 Tokens per Sec:     3855, Lr: 0.000300\n",
            "2020-03-04 17:24:21,887 Epoch   4 Step:    17600 Batch Loss:     2.226798 Tokens per Sec:     4002, Lr: 0.000300\n",
            "2020-03-04 17:25:11,569 Epoch   4 Step:    17700 Batch Loss:     1.987520 Tokens per Sec:     3879, Lr: 0.000300\n",
            "2020-03-04 17:26:00,081 Epoch   4 Step:    17800 Batch Loss:     1.836843 Tokens per Sec:     3983, Lr: 0.000300\n",
            "2020-03-04 17:26:48,455 Epoch   4 Step:    17900 Batch Loss:     2.130673 Tokens per Sec:     4010, Lr: 0.000300\n",
            "2020-03-04 17:27:37,979 Epoch   4 Step:    18000 Batch Loss:     1.857470 Tokens per Sec:     3913, Lr: 0.000300\n",
            "2020-03-04 17:28:29,758 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 17:28:29,758 Saving new checkpoint.\n",
            "2020-03-04 17:28:33,630 Example #0\n",
            "2020-03-04 17:28:33,631 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 17:28:33,631 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 17:28:33,632 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó fún un ní ẹ̀bùn ìràpadà nípasẹ̀ Kristi .\n",
            "2020-03-04 17:28:33,632 Example #1\n",
            "2020-03-04 17:28:33,632 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 17:28:33,632 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 17:28:33,633 \tHypothesis: Ní báyìí , mo ti wá àyè kan láti ṣe iṣẹ́ náà .\n",
            "2020-03-04 17:28:33,633 Example #2\n",
            "2020-03-04 17:28:33,633 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 17:28:33,633 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:28:33,633 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 17:28:33,634 Example #3\n",
            "2020-03-04 17:28:33,634 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 17:28:33,634 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 17:28:33,634 \tHypothesis: Ó ti ní ìrírí púpọ̀ sí i , ó sì ní okun tó lágbára ju bó ṣe yẹ lọ , àmọ́ ó fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 17:28:33,635 Validation result (greedy) at epoch   4, step    18000: bleu:  21.10, loss: 49279.3828, ppl:   7.2592, duration: 55.6550s\n",
            "2020-03-04 17:29:23,050 Epoch   4 Step:    18100 Batch Loss:     1.960684 Tokens per Sec:     3871, Lr: 0.000300\n",
            "2020-03-04 17:30:11,633 Epoch   4 Step:    18200 Batch Loss:     2.017993 Tokens per Sec:     4062, Lr: 0.000300\n",
            "2020-03-04 17:31:00,887 Epoch   4 Step:    18300 Batch Loss:     2.216932 Tokens per Sec:     3908, Lr: 0.000300\n",
            "2020-03-04 17:31:50,638 Epoch   4 Step:    18400 Batch Loss:     2.201126 Tokens per Sec:     3968, Lr: 0.000300\n",
            "2020-03-04 17:32:39,590 Epoch   4 Step:    18500 Batch Loss:     2.091946 Tokens per Sec:     3936, Lr: 0.000300\n",
            "2020-03-04 17:33:29,075 Epoch   4 Step:    18600 Batch Loss:     2.344449 Tokens per Sec:     3932, Lr: 0.000300\n",
            "2020-03-04 17:34:18,781 Epoch   4 Step:    18700 Batch Loss:     2.096700 Tokens per Sec:     3916, Lr: 0.000300\n",
            "2020-03-04 17:35:07,607 Epoch   4 Step:    18800 Batch Loss:     2.096125 Tokens per Sec:     4019, Lr: 0.000300\n",
            "2020-03-04 17:35:57,339 Epoch   4 Step:    18900 Batch Loss:     2.286862 Tokens per Sec:     3989, Lr: 0.000300\n",
            "2020-03-04 17:36:46,031 Epoch   4 Step:    19000 Batch Loss:     1.889821 Tokens per Sec:     4018, Lr: 0.000300\n",
            "2020-03-04 17:37:45,343 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 17:37:45,344 Saving new checkpoint.\n",
            "2020-03-04 17:37:48,660 Example #0\n",
            "2020-03-04 17:37:48,661 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 17:37:48,661 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 17:37:48,661 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ń fúnni ní ẹ̀bùn nípasẹ̀ Kristi .\n",
            "2020-03-04 17:37:48,662 Example #1\n",
            "2020-03-04 17:37:48,662 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 17:37:48,662 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 17:37:48,663 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ kan wà tó yẹ kí n ṣe .\n",
            "2020-03-04 17:37:48,663 Example #2\n",
            "2020-03-04 17:37:48,663 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 17:37:48,663 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:37:48,664 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 17:37:48,664 Example #3\n",
            "2020-03-04 17:37:48,664 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 17:37:48,664 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 17:37:48,665 \tHypothesis: Ó ti ní ìrírí tó pọ̀ jù lọ , ó sì ní okun ju bó o ṣe ń rìn lọ , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 17:37:48,665 Validation result (greedy) at epoch   4, step    19000: bleu:  21.64, loss: 48661.0508, ppl:   7.0809, duration: 62.6332s\n",
            "2020-03-04 17:38:37,857 Epoch   4 Step:    19100 Batch Loss:     2.528663 Tokens per Sec:     3983, Lr: 0.000300\n",
            "2020-03-04 17:39:27,125 Epoch   4 Step:    19200 Batch Loss:     2.067939 Tokens per Sec:     3872, Lr: 0.000300\n",
            "2020-03-04 17:40:16,213 Epoch   4 Step:    19300 Batch Loss:     2.084816 Tokens per Sec:     3929, Lr: 0.000300\n",
            "2020-03-04 17:41:05,194 Epoch   4 Step:    19400 Batch Loss:     2.230633 Tokens per Sec:     4046, Lr: 0.000300\n",
            "2020-03-04 17:41:54,777 Epoch   4 Step:    19500 Batch Loss:     2.100323 Tokens per Sec:     3898, Lr: 0.000300\n",
            "2020-03-04 17:42:44,026 Epoch   4 Step:    19600 Batch Loss:     1.872480 Tokens per Sec:     3942, Lr: 0.000300\n",
            "2020-03-04 17:43:33,185 Epoch   4 Step:    19700 Batch Loss:     2.066259 Tokens per Sec:     3992, Lr: 0.000300\n",
            "2020-03-04 17:44:21,631 Epoch   4 Step:    19800 Batch Loss:     2.184536 Tokens per Sec:     3950, Lr: 0.000300\n",
            "2020-03-04 17:45:10,473 Epoch   4 Step:    19900 Batch Loss:     2.080812 Tokens per Sec:     3962, Lr: 0.000300\n",
            "2020-03-04 17:45:59,936 Epoch   4 Step:    20000 Batch Loss:     2.056551 Tokens per Sec:     3934, Lr: 0.000300\n",
            "2020-03-04 17:46:52,198 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 17:46:52,198 Saving new checkpoint.\n",
            "2020-03-04 17:46:55,994 Example #0\n",
            "2020-03-04 17:46:55,995 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 17:46:55,996 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 17:46:55,996 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 17:46:55,996 Example #1\n",
            "2020-03-04 17:46:55,996 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 17:46:55,997 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 17:46:55,997 \tHypothesis: Mo wá rí i pé mo fẹ́ láti ṣe iṣẹ́ kan .\n",
            "2020-03-04 17:46:55,997 Example #2\n",
            "2020-03-04 17:46:55,997 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 17:46:55,998 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:46:55,998 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 17:46:55,998 Example #3\n",
            "2020-03-04 17:46:55,999 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 17:46:55,999 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 17:46:55,999 \tHypothesis: Ó ti ní ìrírí púpọ̀ sí i , ó sì ń ní okun tó pọ̀ ju bó ṣe yẹ lọ , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 17:46:55,999 Validation result (greedy) at epoch   4, step    20000: bleu:  22.28, loss: 47769.4688, ppl:   6.8315, duration: 56.0626s\n",
            "2020-03-04 17:47:46,160 Epoch   4 Step:    20100 Batch Loss:     1.896048 Tokens per Sec:     3985, Lr: 0.000300\n",
            "2020-03-04 17:48:35,526 Epoch   4 Step:    20200 Batch Loss:     2.001591 Tokens per Sec:     4088, Lr: 0.000300\n",
            "2020-03-04 17:49:24,709 Epoch   4 Step:    20300 Batch Loss:     2.052121 Tokens per Sec:     3886, Lr: 0.000300\n",
            "2020-03-04 17:50:14,651 Epoch   4 Step:    20400 Batch Loss:     2.194220 Tokens per Sec:     3830, Lr: 0.000300\n",
            "2020-03-04 17:51:03,988 Epoch   4 Step:    20500 Batch Loss:     2.117104 Tokens per Sec:     3925, Lr: 0.000300\n",
            "2020-03-04 17:51:52,933 Epoch   4 Step:    20600 Batch Loss:     2.102267 Tokens per Sec:     4006, Lr: 0.000300\n",
            "2020-03-04 17:52:42,899 Epoch   4 Step:    20700 Batch Loss:     2.069510 Tokens per Sec:     3957, Lr: 0.000300\n",
            "2020-03-04 17:53:32,328 Epoch   4 Step:    20800 Batch Loss:     2.110022 Tokens per Sec:     3894, Lr: 0.000300\n",
            "2020-03-04 17:54:21,379 Epoch   4 Step:    20900 Batch Loss:     2.019942 Tokens per Sec:     3969, Lr: 0.000300\n",
            "2020-03-04 17:55:10,731 Epoch   4 Step:    21000 Batch Loss:     2.145467 Tokens per Sec:     3964, Lr: 0.000300\n",
            "2020-03-04 17:56:04,545 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 17:56:04,545 Saving new checkpoint.\n",
            "2020-03-04 17:56:08,121 Example #0\n",
            "2020-03-04 17:56:08,121 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 17:56:08,122 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 17:56:08,122 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tí ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 17:56:08,122 Example #1\n",
            "2020-03-04 17:56:08,123 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 17:56:08,123 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 17:56:08,123 \tHypothesis: Nísinsìnyí , mo ti wá rí i pé mo ní láti máa ṣe iṣẹ́ kan .\n",
            "2020-03-04 17:56:08,123 Example #2\n",
            "2020-03-04 17:56:08,123 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 17:56:08,123 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 17:56:08,123 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 17:56:08,123 Example #3\n",
            "2020-03-04 17:56:08,124 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 17:56:08,124 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 17:56:08,124 \tHypothesis: Ó ti ní ìrírí tó pọ̀ jù , ó sì ń ní okun ju bó ṣe yẹ lọ , àmọ́ ó fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 17:56:08,124 Validation result (greedy) at epoch   4, step    21000: bleu:  22.80, loss: 47556.0977, ppl:   6.7731, duration: 57.3923s\n",
            "2020-03-04 17:56:10,369 Epoch   4: total training loss 10996.81\n",
            "2020-03-04 17:56:10,369 EPOCH 5\n",
            "2020-03-04 17:56:58,719 Epoch   5 Step:    21100 Batch Loss:     1.770952 Tokens per Sec:     3768, Lr: 0.000300\n",
            "2020-03-04 17:57:48,397 Epoch   5 Step:    21200 Batch Loss:     1.912069 Tokens per Sec:     3895, Lr: 0.000300\n",
            "2020-03-04 17:58:37,734 Epoch   5 Step:    21300 Batch Loss:     1.894076 Tokens per Sec:     3883, Lr: 0.000300\n",
            "2020-03-04 17:59:26,700 Epoch   5 Step:    21400 Batch Loss:     1.842078 Tokens per Sec:     3940, Lr: 0.000300\n",
            "2020-03-04 18:00:16,248 Epoch   5 Step:    21500 Batch Loss:     1.880301 Tokens per Sec:     3923, Lr: 0.000300\n",
            "2020-03-04 18:01:06,512 Epoch   5 Step:    21600 Batch Loss:     2.078653 Tokens per Sec:     3943, Lr: 0.000300\n",
            "2020-03-04 18:01:55,453 Epoch   5 Step:    21700 Batch Loss:     2.059693 Tokens per Sec:     4034, Lr: 0.000300\n",
            "2020-03-04 18:02:45,273 Epoch   5 Step:    21800 Batch Loss:     2.036690 Tokens per Sec:     3968, Lr: 0.000300\n",
            "2020-03-04 18:03:33,003 Epoch   5 Step:    21900 Batch Loss:     2.082444 Tokens per Sec:     3982, Lr: 0.000300\n",
            "2020-03-04 18:04:22,225 Epoch   5 Step:    22000 Batch Loss:     1.876489 Tokens per Sec:     3949, Lr: 0.000300\n",
            "2020-03-04 18:06:14,263 Example #0\n",
            "2020-03-04 18:06:14,264 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 18:06:14,265 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 18:06:14,265 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 18:06:14,265 Example #1\n",
            "2020-03-04 18:06:14,266 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 18:06:14,266 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 18:06:14,266 \tHypothesis: Mo ti wá rí i pé mo ní láti ṣe iṣẹ́ kan .\n",
            "2020-03-04 18:06:14,266 Example #2\n",
            "2020-03-04 18:06:14,266 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 18:06:14,267 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 18:06:14,267 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 18:06:14,267 Example #3\n",
            "2020-03-04 18:06:14,267 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 18:06:14,267 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 18:06:14,268 \tHypothesis: Ó ti ní ìrírí tó pọ̀ ju bó o ṣe ń ṣe , àmọ́ ó ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 18:06:14,268 Validation result (greedy) at epoch   5, step    22000: bleu:  22.48, loss: 48029.5156, ppl:   6.9033, duration: 112.0426s\n",
            "2020-03-04 18:07:03,405 Epoch   5 Step:    22100 Batch Loss:     1.899851 Tokens per Sec:     3933, Lr: 0.000300\n",
            "2020-03-04 18:07:52,543 Epoch   5 Step:    22200 Batch Loss:     2.033180 Tokens per Sec:     3920, Lr: 0.000300\n",
            "2020-03-04 18:08:41,861 Epoch   5 Step:    22300 Batch Loss:     1.976553 Tokens per Sec:     3894, Lr: 0.000300\n",
            "2020-03-04 18:09:30,944 Epoch   5 Step:    22400 Batch Loss:     1.884401 Tokens per Sec:     3876, Lr: 0.000300\n",
            "2020-03-04 18:10:20,441 Epoch   5 Step:    22500 Batch Loss:     1.925984 Tokens per Sec:     3905, Lr: 0.000300\n",
            "2020-03-04 18:11:09,751 Epoch   5 Step:    22600 Batch Loss:     2.036601 Tokens per Sec:     3974, Lr: 0.000300\n",
            "2020-03-04 18:11:58,758 Epoch   5 Step:    22700 Batch Loss:     1.888623 Tokens per Sec:     3901, Lr: 0.000300\n",
            "2020-03-04 18:12:48,038 Epoch   5 Step:    22800 Batch Loss:     1.891185 Tokens per Sec:     4049, Lr: 0.000300\n",
            "2020-03-04 18:13:37,276 Epoch   5 Step:    22900 Batch Loss:     1.822821 Tokens per Sec:     3930, Lr: 0.000300\n",
            "2020-03-04 18:14:26,093 Epoch   5 Step:    23000 Batch Loss:     1.856356 Tokens per Sec:     4001, Lr: 0.000300\n",
            "2020-03-04 18:15:21,786 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 18:15:21,787 Saving new checkpoint.\n",
            "2020-03-04 18:15:25,073 Example #0\n",
            "2020-03-04 18:15:25,074 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 18:15:25,074 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 18:15:25,074 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 18:15:25,074 Example #1\n",
            "2020-03-04 18:15:25,075 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 18:15:25,076 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 18:15:25,076 \tHypothesis: Ní báyìí , mo ti wá àyè kan láti ṣe iṣẹ́ .\n",
            "2020-03-04 18:15:25,076 Example #2\n",
            "2020-03-04 18:15:25,076 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 18:15:25,076 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 18:15:25,077 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 18:15:25,077 Example #3\n",
            "2020-03-04 18:15:25,077 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 18:15:25,077 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 18:15:25,077 \tHypothesis: Ó ti ní ìrírí tó pọ̀ jù lọ , ó sì ní okun ju bó o ṣe ń ṣe , àmọ́ ó ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 18:15:25,078 Validation result (greedy) at epoch   5, step    23000: bleu:  23.33, loss: 46747.2031, ppl:   6.5562, duration: 58.9840s\n",
            "2020-03-04 18:16:13,641 Epoch   5 Step:    23100 Batch Loss:     2.033463 Tokens per Sec:     4088, Lr: 0.000300\n",
            "2020-03-04 18:17:03,091 Epoch   5 Step:    23200 Batch Loss:     1.950522 Tokens per Sec:     3935, Lr: 0.000300\n",
            "2020-03-04 18:17:52,192 Epoch   5 Step:    23300 Batch Loss:     2.007885 Tokens per Sec:     4052, Lr: 0.000300\n",
            "2020-03-04 18:18:41,385 Epoch   5 Step:    23400 Batch Loss:     1.877351 Tokens per Sec:     3984, Lr: 0.000300\n",
            "2020-03-04 18:19:29,966 Epoch   5 Step:    23500 Batch Loss:     1.905506 Tokens per Sec:     3942, Lr: 0.000300\n",
            "2020-03-04 18:20:18,419 Epoch   5 Step:    23600 Batch Loss:     2.030661 Tokens per Sec:     3975, Lr: 0.000300\n",
            "2020-03-04 18:21:07,643 Epoch   5 Step:    23700 Batch Loss:     1.873758 Tokens per Sec:     3974, Lr: 0.000300\n",
            "2020-03-04 18:21:55,820 Epoch   5 Step:    23800 Batch Loss:     1.997191 Tokens per Sec:     3975, Lr: 0.000300\n",
            "2020-03-04 18:22:45,506 Epoch   5 Step:    23900 Batch Loss:     1.966687 Tokens per Sec:     3880, Lr: 0.000300\n",
            "2020-03-04 18:23:34,638 Epoch   5 Step:    24000 Batch Loss:     2.000723 Tokens per Sec:     3921, Lr: 0.000300\n",
            "2020-03-04 18:24:33,268 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 18:24:33,269 Saving new checkpoint.\n",
            "2020-03-04 18:24:36,618 Example #0\n",
            "2020-03-04 18:24:36,619 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 18:24:36,619 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 18:24:36,619 \tHypothesis: Ẹni tó ń fúnni ní ẹ̀bùn ìyè , Ẹni tó ń fúnni ní ẹ̀bùn rere nípasẹ̀ Kristi .\n",
            "2020-03-04 18:24:36,620 Example #1\n",
            "2020-03-04 18:24:36,620 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 18:24:36,620 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 18:24:36,620 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ kan wà tó ṣe pàtàkì jù .\n",
            "2020-03-04 18:24:36,621 Example #2\n",
            "2020-03-04 18:24:36,621 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 18:24:36,621 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 18:24:36,621 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 18:24:36,622 Example #3\n",
            "2020-03-04 18:24:36,622 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 18:24:36,622 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 18:24:36,622 \tHypothesis: Ó ti ní ìrírí tó pọ̀ ju bó o ṣe ń ṣe , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 18:24:36,623 Validation result (greedy) at epoch   5, step    24000: bleu:  23.92, loss: 46248.1797, ppl:   6.4260, duration: 61.9844s\n",
            "2020-03-04 18:25:25,549 Epoch   5 Step:    24100 Batch Loss:     2.109005 Tokens per Sec:     3972, Lr: 0.000300\n",
            "2020-03-04 18:26:14,580 Epoch   5 Step:    24200 Batch Loss:     2.379462 Tokens per Sec:     3868, Lr: 0.000300\n",
            "2020-03-04 18:27:04,616 Epoch   5 Step:    24300 Batch Loss:     2.123616 Tokens per Sec:     3962, Lr: 0.000300\n",
            "2020-03-04 18:27:53,511 Epoch   5 Step:    24400 Batch Loss:     1.978261 Tokens per Sec:     4083, Lr: 0.000300\n",
            "2020-03-04 18:28:42,764 Epoch   5 Step:    24500 Batch Loss:     2.106839 Tokens per Sec:     3948, Lr: 0.000300\n",
            "2020-03-04 18:29:31,644 Epoch   5 Step:    24600 Batch Loss:     1.689650 Tokens per Sec:     3919, Lr: 0.000300\n",
            "2020-03-04 18:30:20,173 Epoch   5 Step:    24700 Batch Loss:     1.779446 Tokens per Sec:     4000, Lr: 0.000300\n",
            "2020-03-04 18:31:08,333 Epoch   5 Step:    24800 Batch Loss:     1.980498 Tokens per Sec:     3907, Lr: 0.000300\n",
            "2020-03-04 18:31:57,632 Epoch   5 Step:    24900 Batch Loss:     2.034642 Tokens per Sec:     3981, Lr: 0.000300\n",
            "2020-03-04 18:32:47,148 Epoch   5 Step:    25000 Batch Loss:     2.062139 Tokens per Sec:     3900, Lr: 0.000300\n",
            "2020-03-04 18:33:42,727 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 18:33:42,727 Saving new checkpoint.\n",
            "2020-03-04 18:33:46,147 Example #0\n",
            "2020-03-04 18:33:46,148 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 18:33:46,148 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 18:33:46,148 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn rere nípasẹ̀ Kristi .\n",
            "2020-03-04 18:33:46,148 Example #1\n",
            "2020-03-04 18:33:46,148 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 18:33:46,148 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 18:33:46,148 \tHypothesis: Ní báyìí , mo ní láti rí i pé iṣẹ́ tó yẹ kí n ṣe ni mo fẹ́ ṣe .\n",
            "2020-03-04 18:33:46,148 Example #2\n",
            "2020-03-04 18:33:46,149 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 18:33:46,149 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 18:33:46,149 \tHypothesis: Ṣé mo mọyì àwọn nǹkan ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 18:33:46,149 Example #3\n",
            "2020-03-04 18:33:46,149 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 18:33:46,149 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 18:33:46,149 \tHypothesis: Ó ti ní ìrírí tó pọ̀ ju bó o ṣe ń ṣe , àmọ́ ó fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 18:33:46,149 Validation result (greedy) at epoch   5, step    25000: bleu:  24.64, loss: 45661.5312, ppl:   6.2761, duration: 59.0013s\n",
            "2020-03-04 18:34:35,877 Epoch   5 Step:    25100 Batch Loss:     1.787851 Tokens per Sec:     3969, Lr: 0.000300\n",
            "2020-03-04 18:35:24,457 Epoch   5 Step:    25200 Batch Loss:     1.930926 Tokens per Sec:     3983, Lr: 0.000300\n",
            "2020-03-04 18:36:14,102 Epoch   5 Step:    25300 Batch Loss:     2.068879 Tokens per Sec:     3878, Lr: 0.000300\n",
            "2020-03-04 18:37:03,716 Epoch   5 Step:    25400 Batch Loss:     2.043938 Tokens per Sec:     3993, Lr: 0.000300\n",
            "2020-03-04 18:37:52,656 Epoch   5 Step:    25500 Batch Loss:     1.890001 Tokens per Sec:     4029, Lr: 0.000300\n",
            "2020-03-04 18:38:41,217 Epoch   5 Step:    25600 Batch Loss:     1.826350 Tokens per Sec:     3941, Lr: 0.000300\n",
            "2020-03-04 18:39:30,414 Epoch   5 Step:    25700 Batch Loss:     2.022624 Tokens per Sec:     3998, Lr: 0.000300\n",
            "2020-03-04 18:40:19,706 Epoch   5 Step:    25800 Batch Loss:     1.883567 Tokens per Sec:     3906, Lr: 0.000300\n",
            "2020-03-04 18:41:08,609 Epoch   5 Step:    25900 Batch Loss:     2.051877 Tokens per Sec:     3880, Lr: 0.000300\n",
            "2020-03-04 18:41:57,609 Epoch   5 Step:    26000 Batch Loss:     2.019619 Tokens per Sec:     3900, Lr: 0.000300\n",
            "2020-03-04 18:42:53,234 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 18:42:53,235 Saving new checkpoint.\n",
            "2020-03-04 18:42:56,907 Example #0\n",
            "2020-03-04 18:42:56,908 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 18:42:56,908 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 18:42:56,908 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 18:42:56,908 Example #1\n",
            "2020-03-04 18:42:56,909 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 18:42:56,909 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 18:42:56,909 \tHypothesis: Mo ti wá rí i pé iṣẹ́ tó yẹ kí n ṣe ni mo máa ń ṣe .\n",
            "2020-03-04 18:42:56,910 Example #2\n",
            "2020-03-04 18:42:56,910 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 18:42:56,910 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 18:42:56,910 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-03-04 18:42:56,911 Example #3\n",
            "2020-03-04 18:42:56,911 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 18:42:56,911 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 18:42:56,911 \tHypothesis: Ó ti ní ìrírí tó pọ̀ jù , ó sì ń jẹ́ kó o lè máa ṣe ohun tó wù ẹ́ , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 18:42:56,912 Validation result (greedy) at epoch   5, step    26000: bleu:  24.04, loss: 45456.7852, ppl:   6.2246, duration: 59.3017s\n",
            "2020-03-04 18:43:46,015 Epoch   5 Step:    26100 Batch Loss:     2.000589 Tokens per Sec:     3883, Lr: 0.000300\n",
            "2020-03-04 18:44:34,340 Epoch   5 Step:    26200 Batch Loss:     1.942445 Tokens per Sec:     4108, Lr: 0.000300\n",
            "2020-03-04 18:45:04,403 Epoch   5: total training loss 10325.09\n",
            "2020-03-04 18:45:04,403 EPOCH 6\n",
            "2020-03-04 18:45:23,901 Epoch   6 Step:    26300 Batch Loss:     1.856426 Tokens per Sec:     3817, Lr: 0.000300\n",
            "2020-03-04 18:46:13,446 Epoch   6 Step:    26400 Batch Loss:     1.646287 Tokens per Sec:     3873, Lr: 0.000300\n",
            "2020-03-04 18:47:02,383 Epoch   6 Step:    26500 Batch Loss:     1.805441 Tokens per Sec:     3880, Lr: 0.000300\n",
            "2020-03-04 18:47:51,816 Epoch   6 Step:    26600 Batch Loss:     1.975955 Tokens per Sec:     3872, Lr: 0.000300\n",
            "2020-03-04 18:48:40,928 Epoch   6 Step:    26700 Batch Loss:     1.956282 Tokens per Sec:     3905, Lr: 0.000300\n",
            "2020-03-04 18:49:30,063 Epoch   6 Step:    26800 Batch Loss:     2.010087 Tokens per Sec:     3988, Lr: 0.000300\n",
            "2020-03-04 18:50:19,398 Epoch   6 Step:    26900 Batch Loss:     1.937109 Tokens per Sec:     3962, Lr: 0.000300\n",
            "2020-03-04 18:51:08,130 Epoch   6 Step:    27000 Batch Loss:     1.956031 Tokens per Sec:     3904, Lr: 0.000300\n",
            "2020-03-04 18:52:03,309 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 18:52:03,309 Saving new checkpoint.\n",
            "2020-03-04 18:52:06,863 Example #0\n",
            "2020-03-04 18:52:06,864 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 18:52:06,865 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 18:52:06,865 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó fún un ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 18:52:06,865 Example #1\n",
            "2020-03-04 18:52:06,866 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 18:52:06,866 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 18:52:06,866 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó yẹ kí n ṣe ni mo máa ń ṣe .\n",
            "2020-03-04 18:52:06,866 Example #2\n",
            "2020-03-04 18:52:06,867 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 18:52:06,867 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 18:52:06,867 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 18:52:06,867 Example #3\n",
            "2020-03-04 18:52:06,867 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 18:52:06,868 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 18:52:06,868 \tHypothesis: Ó ti ní ìrírí tó pọ̀ ju ti tẹ́lẹ̀ lọ , ó sì ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 18:52:06,868 Validation result (greedy) at epoch   6, step    27000: bleu:  24.94, loss: 45325.1484, ppl:   6.1917, duration: 58.7371s\n",
            "2020-03-04 18:52:56,426 Epoch   6 Step:    27100 Batch Loss:     1.992890 Tokens per Sec:     4014, Lr: 0.000300\n",
            "2020-03-04 18:53:45,355 Epoch   6 Step:    27200 Batch Loss:     1.925340 Tokens per Sec:     3957, Lr: 0.000300\n",
            "2020-03-04 18:54:34,694 Epoch   6 Step:    27300 Batch Loss:     1.732107 Tokens per Sec:     3991, Lr: 0.000300\n",
            "2020-03-04 18:55:24,126 Epoch   6 Step:    27400 Batch Loss:     1.655993 Tokens per Sec:     3981, Lr: 0.000300\n",
            "2020-03-04 18:56:11,863 Epoch   6 Step:    27500 Batch Loss:     1.674577 Tokens per Sec:     4042, Lr: 0.000300\n",
            "2020-03-04 18:57:00,607 Epoch   6 Step:    27600 Batch Loss:     1.870956 Tokens per Sec:     4008, Lr: 0.000300\n",
            "2020-03-04 18:57:48,490 Epoch   6 Step:    27700 Batch Loss:     1.908953 Tokens per Sec:     3909, Lr: 0.000300\n",
            "2020-03-04 18:58:38,516 Epoch   6 Step:    27800 Batch Loss:     1.546577 Tokens per Sec:     3906, Lr: 0.000300\n",
            "2020-03-04 18:59:27,725 Epoch   6 Step:    27900 Batch Loss:     1.901744 Tokens per Sec:     3937, Lr: 0.000300\n",
            "2020-03-04 19:00:16,873 Epoch   6 Step:    28000 Batch Loss:     1.869847 Tokens per Sec:     3902, Lr: 0.000300\n",
            "2020-03-04 19:01:16,264 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 19:01:16,264 Saving new checkpoint.\n",
            "2020-03-04 19:01:20,158 Example #0\n",
            "2020-03-04 19:01:20,159 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 19:01:20,159 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 19:01:20,159 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 19:01:20,160 Example #1\n",
            "2020-03-04 19:01:20,160 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 19:01:20,160 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 19:01:20,160 \tHypothesis: Mo wá rí i pé iṣẹ́ abẹ kan wà tó yẹ kí n ṣe .\n",
            "2020-03-04 19:01:20,161 Example #2\n",
            "2020-03-04 19:01:20,162 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 19:01:20,162 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 19:01:20,162 \tHypothesis: Ṣé mo mọyì àwọn nǹkan ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 19:01:20,162 Example #3\n",
            "2020-03-04 19:01:20,163 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 19:01:20,163 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 19:01:20,163 \tHypothesis: Ó ní ìrírí tó pọ̀ ju bó o ṣe ń ṣe , ó sì ń lágbára ju bó o ṣe ń ṣe , àmọ́ ó fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 19:01:20,163 Validation result (greedy) at epoch   6, step    28000: bleu:  25.65, loss: 44830.2930, ppl:   6.0697, duration: 63.2898s\n",
            "2020-03-04 19:02:09,624 Epoch   6 Step:    28100 Batch Loss:     1.670866 Tokens per Sec:     3844, Lr: 0.000300\n",
            "2020-03-04 19:02:58,721 Epoch   6 Step:    28200 Batch Loss:     1.784734 Tokens per Sec:     4016, Lr: 0.000300\n",
            "2020-03-04 19:03:48,028 Epoch   6 Step:    28300 Batch Loss:     1.805056 Tokens per Sec:     3992, Lr: 0.000300\n",
            "2020-03-04 19:04:37,859 Epoch   6 Step:    28400 Batch Loss:     1.796957 Tokens per Sec:     3880, Lr: 0.000300\n",
            "2020-03-04 19:05:27,108 Epoch   6 Step:    28500 Batch Loss:     2.177529 Tokens per Sec:     3921, Lr: 0.000300\n",
            "2020-03-04 19:06:16,575 Epoch   6 Step:    28600 Batch Loss:     1.974440 Tokens per Sec:     4008, Lr: 0.000300\n",
            "2020-03-04 19:07:05,660 Epoch   6 Step:    28700 Batch Loss:     1.757527 Tokens per Sec:     3958, Lr: 0.000300\n",
            "2020-03-04 19:07:55,011 Epoch   6 Step:    28800 Batch Loss:     1.805969 Tokens per Sec:     3989, Lr: 0.000300\n",
            "2020-03-04 19:08:43,677 Epoch   6 Step:    28900 Batch Loss:     2.052729 Tokens per Sec:     3839, Lr: 0.000300\n",
            "2020-03-04 19:09:32,993 Epoch   6 Step:    29000 Batch Loss:     1.821401 Tokens per Sec:     3921, Lr: 0.000300\n",
            "2020-03-04 19:10:26,604 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 19:10:26,605 Saving new checkpoint.\n",
            "2020-03-04 19:10:29,903 Example #0\n",
            "2020-03-04 19:10:29,903 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 19:10:29,903 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 19:10:29,903 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 19:10:29,903 Example #1\n",
            "2020-03-04 19:10:29,904 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 19:10:29,904 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 19:10:29,904 \tHypothesis: Mo wá rí i pé ó yẹ kí n wá iṣẹ́ tó bójú mu .\n",
            "2020-03-04 19:10:29,904 Example #2\n",
            "2020-03-04 19:10:29,904 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 19:10:29,904 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 19:10:29,904 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-03-04 19:10:29,905 Example #3\n",
            "2020-03-04 19:10:29,905 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 19:10:29,905 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 19:10:29,905 \tHypothesis: Ó ní ìrírí tó pọ̀ ju bó o ṣe ń ṣe , àmọ́ ó fi sùúrù rìn jìnnà sí ẹ .\n",
            "2020-03-04 19:10:29,905 Validation result (greedy) at epoch   6, step    29000: bleu:  24.49, loss: 44760.8789, ppl:   6.0528, duration: 56.9119s\n",
            "2020-03-04 19:11:20,409 Epoch   6 Step:    29100 Batch Loss:     1.902130 Tokens per Sec:     3914, Lr: 0.000300\n",
            "2020-03-04 19:12:09,389 Epoch   6 Step:    29200 Batch Loss:     1.773835 Tokens per Sec:     3857, Lr: 0.000300\n",
            "2020-03-04 19:12:58,350 Epoch   6 Step:    29300 Batch Loss:     1.843428 Tokens per Sec:     3989, Lr: 0.000300\n",
            "2020-03-04 19:13:47,355 Epoch   6 Step:    29400 Batch Loss:     1.771064 Tokens per Sec:     3937, Lr: 0.000300\n",
            "2020-03-04 19:14:37,213 Epoch   6 Step:    29500 Batch Loss:     1.837373 Tokens per Sec:     3892, Lr: 0.000300\n",
            "2020-03-04 19:15:26,964 Epoch   6 Step:    29600 Batch Loss:     2.104223 Tokens per Sec:     3942, Lr: 0.000300\n",
            "2020-03-04 19:16:16,488 Epoch   6 Step:    29700 Batch Loss:     1.761016 Tokens per Sec:     3908, Lr: 0.000300\n",
            "2020-03-04 19:17:05,420 Epoch   6 Step:    29800 Batch Loss:     1.868052 Tokens per Sec:     3984, Lr: 0.000300\n",
            "2020-03-04 19:17:54,804 Epoch   6 Step:    29900 Batch Loss:     1.733474 Tokens per Sec:     3890, Lr: 0.000300\n",
            "2020-03-04 19:18:43,945 Epoch   6 Step:    30000 Batch Loss:     1.737345 Tokens per Sec:     4007, Lr: 0.000300\n",
            "2020-03-04 19:19:36,369 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 19:19:36,369 Saving new checkpoint.\n",
            "2020-03-04 19:19:40,213 Example #0\n",
            "2020-03-04 19:19:40,214 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 19:19:40,214 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 19:19:40,214 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tó fún un ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 19:19:40,214 Example #1\n",
            "2020-03-04 19:19:40,215 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 19:19:40,215 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 19:19:40,215 \tHypothesis: Mo ti wá rí i pé iṣẹ́ tó yẹ kí n ṣe ni .\n",
            "2020-03-04 19:19:40,215 Example #2\n",
            "2020-03-04 19:19:40,215 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 19:19:40,216 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 19:19:40,216 \tHypothesis: Ṣé mo mọyì àwọn nǹkan ìní mi ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 19:19:40,216 Example #3\n",
            "2020-03-04 19:19:40,216 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 19:19:40,217 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 19:19:40,217 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì lágbára ju bó o ṣe ń ṣe , àmọ́ ó fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 19:19:40,217 Validation result (greedy) at epoch   6, step    30000: bleu:  25.06, loss: 44387.2500, ppl:   5.9625, duration: 56.2712s\n",
            "2020-03-04 19:20:30,245 Epoch   6 Step:    30100 Batch Loss:     1.948338 Tokens per Sec:     3810, Lr: 0.000300\n",
            "2020-03-04 19:21:19,487 Epoch   6 Step:    30200 Batch Loss:     1.998239 Tokens per Sec:     4003, Lr: 0.000300\n",
            "2020-03-04 19:22:09,012 Epoch   6 Step:    30300 Batch Loss:     1.783731 Tokens per Sec:     3968, Lr: 0.000300\n",
            "2020-03-04 19:22:58,235 Epoch   6 Step:    30400 Batch Loss:     1.705459 Tokens per Sec:     3946, Lr: 0.000300\n",
            "2020-03-04 19:23:46,884 Epoch   6 Step:    30500 Batch Loss:     2.314542 Tokens per Sec:     3940, Lr: 0.000300\n",
            "2020-03-04 19:24:36,270 Epoch   6 Step:    30600 Batch Loss:     1.862667 Tokens per Sec:     3931, Lr: 0.000300\n",
            "2020-03-04 19:25:25,151 Epoch   6 Step:    30700 Batch Loss:     1.787245 Tokens per Sec:     3965, Lr: 0.000300\n",
            "2020-03-04 19:26:14,251 Epoch   6 Step:    30800 Batch Loss:     1.858253 Tokens per Sec:     4005, Lr: 0.000300\n",
            "2020-03-04 19:27:04,082 Epoch   6 Step:    30900 Batch Loss:     2.014626 Tokens per Sec:     3896, Lr: 0.000300\n",
            "2020-03-04 19:27:53,054 Epoch   6 Step:    31000 Batch Loss:     1.857952 Tokens per Sec:     3982, Lr: 0.000300\n",
            "2020-03-04 19:29:00,942 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 19:29:00,943 Saving new checkpoint.\n",
            "2020-03-04 19:29:04,825 Example #0\n",
            "2020-03-04 19:29:04,826 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 19:29:04,826 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 19:29:04,826 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 19:29:04,826 Example #1\n",
            "2020-03-04 19:29:04,827 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 19:29:04,827 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 19:29:04,827 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó dáa ni .\n",
            "2020-03-04 19:29:04,827 Example #2\n",
            "2020-03-04 19:29:04,828 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 19:29:04,828 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 19:29:04,828 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 19:29:04,828 Example #3\n",
            "2020-03-04 19:29:04,829 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 19:29:04,829 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 19:29:04,829 \tHypothesis: Ó ní ìrírí tó pọ̀ ju ti tẹ́lẹ̀ lọ , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 19:29:04,829 Validation result (greedy) at epoch   6, step    31000: bleu:  25.73, loss: 44096.4727, ppl:   5.8932, duration: 71.7752s\n",
            "2020-03-04 19:29:54,320 Epoch   6 Step:    31100 Batch Loss:     2.089112 Tokens per Sec:     3869, Lr: 0.000300\n",
            "2020-03-04 19:30:43,530 Epoch   6 Step:    31200 Batch Loss:     1.875472 Tokens per Sec:     3991, Lr: 0.000300\n",
            "2020-03-04 19:31:32,369 Epoch   6 Step:    31300 Batch Loss:     1.802309 Tokens per Sec:     3939, Lr: 0.000300\n",
            "2020-03-04 19:32:21,785 Epoch   6 Step:    31400 Batch Loss:     1.570454 Tokens per Sec:     3932, Lr: 0.000300\n",
            "2020-03-04 19:33:10,642 Epoch   6 Step:    31500 Batch Loss:     1.763397 Tokens per Sec:     3972, Lr: 0.000300\n",
            "2020-03-04 19:33:20,918 Epoch   6: total training loss 9832.39\n",
            "2020-03-04 19:33:20,918 EPOCH 7\n",
            "2020-03-04 19:34:00,074 Epoch   7 Step:    31600 Batch Loss:     1.748589 Tokens per Sec:     3891, Lr: 0.000300\n",
            "2020-03-04 19:34:48,961 Epoch   7 Step:    31700 Batch Loss:     1.760257 Tokens per Sec:     3848, Lr: 0.000300\n",
            "2020-03-04 19:35:38,685 Epoch   7 Step:    31800 Batch Loss:     1.812397 Tokens per Sec:     4015, Lr: 0.000300\n",
            "2020-03-04 19:36:27,444 Epoch   7 Step:    31900 Batch Loss:     1.659035 Tokens per Sec:     3851, Lr: 0.000300\n",
            "2020-03-04 19:37:16,507 Epoch   7 Step:    32000 Batch Loss:     1.726579 Tokens per Sec:     4044, Lr: 0.000300\n",
            "2020-03-04 19:38:14,088 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 19:38:14,088 Saving new checkpoint.\n",
            "2020-03-04 19:38:17,801 Example #0\n",
            "2020-03-04 19:38:17,802 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 19:38:17,802 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 19:38:17,802 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 19:38:17,802 Example #1\n",
            "2020-03-04 19:38:17,803 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 19:38:17,803 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 19:38:17,803 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó yẹ kí n ṣe ni mo máa ń ṣe .\n",
            "2020-03-04 19:38:17,803 Example #2\n",
            "2020-03-04 19:38:17,803 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 19:38:17,803 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 19:38:17,803 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 19:38:17,803 Example #3\n",
            "2020-03-04 19:38:17,804 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 19:38:17,804 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 19:38:17,804 \tHypothesis: Ó ní ìrírí tó pọ̀ ju ti tẹ́lẹ̀ lọ , ó sì lókun ju bó o ṣe ń ṣe , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 19:38:17,804 Validation result (greedy) at epoch   7, step    32000: bleu:  26.10, loss: 43940.6250, ppl:   5.8563, duration: 61.2966s\n",
            "2020-03-04 19:39:07,370 Epoch   7 Step:    32100 Batch Loss:     1.622103 Tokens per Sec:     3904, Lr: 0.000300\n",
            "2020-03-04 19:39:56,671 Epoch   7 Step:    32200 Batch Loss:     1.998254 Tokens per Sec:     3923, Lr: 0.000300\n",
            "2020-03-04 19:40:46,388 Epoch   7 Step:    32300 Batch Loss:     1.864057 Tokens per Sec:     3925, Lr: 0.000300\n",
            "2020-03-04 19:41:35,517 Epoch   7 Step:    32400 Batch Loss:     1.956273 Tokens per Sec:     3935, Lr: 0.000300\n",
            "2020-03-04 19:42:24,944 Epoch   7 Step:    32500 Batch Loss:     1.723511 Tokens per Sec:     3981, Lr: 0.000300\n",
            "2020-03-04 19:43:13,736 Epoch   7 Step:    32600 Batch Loss:     1.830286 Tokens per Sec:     3973, Lr: 0.000300\n",
            "2020-03-04 19:44:03,459 Epoch   7 Step:    32700 Batch Loss:     1.881955 Tokens per Sec:     3959, Lr: 0.000300\n",
            "2020-03-04 19:44:52,955 Epoch   7 Step:    32800 Batch Loss:     1.900553 Tokens per Sec:     3912, Lr: 0.000300\n",
            "2020-03-04 19:45:42,155 Epoch   7 Step:    32900 Batch Loss:     1.858664 Tokens per Sec:     3914, Lr: 0.000300\n",
            "2020-03-04 19:46:31,194 Epoch   7 Step:    33000 Batch Loss:     1.959264 Tokens per Sec:     3931, Lr: 0.000300\n",
            "2020-03-04 19:47:35,653 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 19:47:35,653 Saving new checkpoint.\n",
            "2020-03-04 19:47:38,978 Example #0\n",
            "2020-03-04 19:47:38,979 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 19:47:38,979 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 19:47:38,980 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 19:47:38,980 Example #1\n",
            "2020-03-04 19:47:38,980 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 19:47:38,980 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 19:47:38,981 \tHypothesis: Mo wá rí i pé mo ti wá àyè láti ríṣẹ́ .\n",
            "2020-03-04 19:47:38,981 Example #2\n",
            "2020-03-04 19:47:38,982 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 19:47:38,982 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 19:47:38,982 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn lọ ?\n",
            "2020-03-04 19:47:38,983 Example #3\n",
            "2020-03-04 19:47:38,983 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 19:47:38,983 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 19:47:38,983 \tHypothesis: Ó ní ìrírí tó pọ̀ ju ti tẹ́lẹ̀ lọ , ó sì lókun ju bó o ṣe ń ṣe , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 19:47:38,983 Validation result (greedy) at epoch   7, step    33000: bleu:  25.84, loss: 43693.8398, ppl:   5.7985, duration: 67.7894s\n",
            "2020-03-04 19:48:28,402 Epoch   7 Step:    33100 Batch Loss:     1.918397 Tokens per Sec:     3940, Lr: 0.000300\n",
            "2020-03-04 19:49:17,904 Epoch   7 Step:    33200 Batch Loss:     1.832773 Tokens per Sec:     3999, Lr: 0.000300\n",
            "2020-03-04 19:50:07,971 Epoch   7 Step:    33300 Batch Loss:     1.927829 Tokens per Sec:     3897, Lr: 0.000300\n",
            "2020-03-04 19:50:56,839 Epoch   7 Step:    33400 Batch Loss:     1.856377 Tokens per Sec:     3950, Lr: 0.000300\n",
            "2020-03-04 19:51:45,768 Epoch   7 Step:    33500 Batch Loss:     1.889584 Tokens per Sec:     3887, Lr: 0.000300\n",
            "2020-03-04 19:52:34,425 Epoch   7 Step:    33600 Batch Loss:     1.971363 Tokens per Sec:     3930, Lr: 0.000300\n",
            "2020-03-04 19:53:23,872 Epoch   7 Step:    33700 Batch Loss:     1.876507 Tokens per Sec:     3896, Lr: 0.000300\n",
            "2020-03-04 19:54:12,685 Epoch   7 Step:    33800 Batch Loss:     1.788844 Tokens per Sec:     3987, Lr: 0.000300\n",
            "2020-03-04 19:55:02,096 Epoch   7 Step:    33900 Batch Loss:     1.873505 Tokens per Sec:     3918, Lr: 0.000300\n",
            "2020-03-04 19:55:51,376 Epoch   7 Step:    34000 Batch Loss:     1.908825 Tokens per Sec:     3968, Lr: 0.000300\n",
            "2020-03-04 19:56:47,816 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 19:56:47,817 Saving new checkpoint.\n",
            "2020-03-04 19:56:51,404 Example #0\n",
            "2020-03-04 19:56:51,405 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 19:56:51,405 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 19:56:51,405 \tHypothesis: Ẹni tó ń gbé ìwàláàyè ni Orísun ìyè , Ẹni tó ń fún un ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 19:56:51,405 Example #1\n",
            "2020-03-04 19:56:51,406 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 19:56:51,406 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 19:56:51,406 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó yẹ kí n ṣe ni .\n",
            "2020-03-04 19:56:51,406 Example #2\n",
            "2020-03-04 19:56:51,406 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 19:56:51,406 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 19:56:51,406 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 19:56:51,406 Example #3\n",
            "2020-03-04 19:56:51,407 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 19:56:51,407 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 19:56:51,407 \tHypothesis: Ó ní ìrírí tó pọ̀ ju bó o ṣe ń ṣe , àmọ́ ó fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 19:56:51,407 Validation result (greedy) at epoch   7, step    34000: bleu:  25.78, loss: 43429.4141, ppl:   5.7371, duration: 60.0304s\n",
            "2020-03-04 19:57:40,288 Epoch   7 Step:    34100 Batch Loss:     1.922879 Tokens per Sec:     3972, Lr: 0.000300\n",
            "2020-03-04 19:58:28,471 Epoch   7 Step:    34200 Batch Loss:     1.841866 Tokens per Sec:     3920, Lr: 0.000300\n",
            "2020-03-04 19:59:17,834 Epoch   7 Step:    34300 Batch Loss:     1.914990 Tokens per Sec:     3893, Lr: 0.000300\n",
            "2020-03-04 20:00:07,229 Epoch   7 Step:    34400 Batch Loss:     1.930659 Tokens per Sec:     3975, Lr: 0.000300\n",
            "2020-03-04 20:00:55,440 Epoch   7 Step:    34500 Batch Loss:     1.828187 Tokens per Sec:     4007, Lr: 0.000300\n",
            "2020-03-04 20:01:44,886 Epoch   7 Step:    34600 Batch Loss:     1.688663 Tokens per Sec:     4004, Lr: 0.000300\n",
            "2020-03-04 20:02:34,054 Epoch   7 Step:    34700 Batch Loss:     1.857305 Tokens per Sec:     3860, Lr: 0.000300\n",
            "2020-03-04 20:03:23,356 Epoch   7 Step:    34800 Batch Loss:     1.773076 Tokens per Sec:     4010, Lr: 0.000300\n",
            "2020-03-04 20:04:12,303 Epoch   7 Step:    34900 Batch Loss:     2.001942 Tokens per Sec:     3941, Lr: 0.000300\n",
            "2020-03-04 20:05:01,333 Epoch   7 Step:    35000 Batch Loss:     2.061043 Tokens per Sec:     3998, Lr: 0.000300\n",
            "2020-03-04 20:06:00,764 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 20:06:00,765 Saving new checkpoint.\n",
            "2020-03-04 20:06:04,091 Example #0\n",
            "2020-03-04 20:06:04,092 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 20:06:04,092 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 20:06:04,092 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 20:06:04,092 Example #1\n",
            "2020-03-04 20:06:04,092 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 20:06:04,093 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 20:06:04,093 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó bófin mu ni .\n",
            "2020-03-04 20:06:04,093 Example #2\n",
            "2020-03-04 20:06:04,093 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 20:06:04,093 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 20:06:04,093 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 20:06:04,094 Example #3\n",
            "2020-03-04 20:06:04,094 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 20:06:04,094 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 20:06:04,094 \tHypothesis: Ó ti ní ìrírí tó pọ̀ ju ti tẹ́lẹ̀ lọ , ó sì lágbára ju bó ṣe yẹ lọ , àmọ́ ó fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 20:06:04,094 Validation result (greedy) at epoch   7, step    35000: bleu:  26.19, loss: 43144.3789, ppl:   5.6717, duration: 62.7608s\n",
            "2020-03-04 20:06:54,503 Epoch   7 Step:    35100 Batch Loss:     1.816088 Tokens per Sec:     3818, Lr: 0.000300\n",
            "2020-03-04 20:07:44,156 Epoch   7 Step:    35200 Batch Loss:     1.939116 Tokens per Sec:     4016, Lr: 0.000300\n",
            "2020-03-04 20:08:33,742 Epoch   7 Step:    35300 Batch Loss:     1.683541 Tokens per Sec:     3971, Lr: 0.000300\n",
            "2020-03-04 20:09:22,880 Epoch   7 Step:    35400 Batch Loss:     1.824999 Tokens per Sec:     3988, Lr: 0.000300\n",
            "2020-03-04 20:10:12,138 Epoch   7 Step:    35500 Batch Loss:     1.764810 Tokens per Sec:     3969, Lr: 0.000300\n",
            "2020-03-04 20:11:01,532 Epoch   7 Step:    35600 Batch Loss:     1.802957 Tokens per Sec:     3844, Lr: 0.000300\n",
            "2020-03-04 20:11:50,394 Epoch   7 Step:    35700 Batch Loss:     1.734446 Tokens per Sec:     3880, Lr: 0.000300\n",
            "2020-03-04 20:12:40,164 Epoch   7 Step:    35800 Batch Loss:     1.856081 Tokens per Sec:     3943, Lr: 0.000300\n",
            "2020-03-04 20:13:29,527 Epoch   7 Step:    35900 Batch Loss:     1.791671 Tokens per Sec:     3913, Lr: 0.000300\n",
            "2020-03-04 20:14:18,943 Epoch   7 Step:    36000 Batch Loss:     1.676998 Tokens per Sec:     3955, Lr: 0.000300\n",
            "2020-03-04 20:15:08,021 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 20:15:08,022 Saving new checkpoint.\n",
            "2020-03-04 20:15:11,336 Example #0\n",
            "2020-03-04 20:15:11,336 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 20:15:11,337 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 20:15:11,337 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tó ń fúnni ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 20:15:11,338 Example #1\n",
            "2020-03-04 20:15:11,338 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 20:15:11,338 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 20:15:11,338 \tHypothesis: Mo ti wá rí i pé ó yẹ kí n wá iṣẹ́ tó bófin mu .\n",
            "2020-03-04 20:15:11,338 Example #2\n",
            "2020-03-04 20:15:11,339 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 20:15:11,339 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 20:15:11,339 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-03-04 20:15:11,340 Example #3\n",
            "2020-03-04 20:15:11,340 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 20:15:11,340 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 20:15:11,340 \tHypothesis: Ó ní ìrírí tó pọ̀ ju bó o ṣe ń ṣe , àmọ́ ó máa ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 20:15:11,341 Validation result (greedy) at epoch   7, step    36000: bleu:  25.95, loss: 43116.2500, ppl:   5.6653, duration: 52.3972s\n",
            "2020-03-04 20:16:00,986 Epoch   7 Step:    36100 Batch Loss:     1.626101 Tokens per Sec:     3862, Lr: 0.000300\n",
            "2020-03-04 20:16:50,075 Epoch   7 Step:    36200 Batch Loss:     1.624523 Tokens per Sec:     3934, Lr: 0.000300\n",
            "2020-03-04 20:17:39,585 Epoch   7 Step:    36300 Batch Loss:     1.736853 Tokens per Sec:     3993, Lr: 0.000300\n",
            "2020-03-04 20:18:28,351 Epoch   7 Step:    36400 Batch Loss:     1.594271 Tokens per Sec:     3944, Lr: 0.000300\n",
            "2020-03-04 20:19:18,249 Epoch   7 Step:    36500 Batch Loss:     1.744062 Tokens per Sec:     3923, Lr: 0.000300\n",
            "2020-03-04 20:20:07,764 Epoch   7 Step:    36600 Batch Loss:     1.720792 Tokens per Sec:     3872, Lr: 0.000300\n",
            "2020-03-04 20:20:57,319 Epoch   7 Step:    36700 Batch Loss:     1.668262 Tokens per Sec:     4018, Lr: 0.000300\n",
            "2020-03-04 20:21:34,744 Epoch   7: total training loss 9453.90\n",
            "2020-03-04 20:21:34,744 EPOCH 8\n",
            "2020-03-04 20:21:47,167 Epoch   8 Step:    36800 Batch Loss:     1.912444 Tokens per Sec:     3834, Lr: 0.000300\n",
            "2020-03-04 20:22:36,322 Epoch   8 Step:    36900 Batch Loss:     1.870399 Tokens per Sec:     4040, Lr: 0.000300\n",
            "2020-03-04 20:23:25,192 Epoch   8 Step:    37000 Batch Loss:     1.575200 Tokens per Sec:     3894, Lr: 0.000300\n",
            "2020-03-04 20:24:27,148 Hooray! New best validation result [ppl]!\n",
            "2020-03-04 20:24:27,148 Saving new checkpoint.\n",
            "2020-03-04 20:24:30,480 Example #0\n",
            "2020-03-04 20:24:30,484 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-03-04 20:24:30,484 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-03-04 20:24:30,484 \tHypothesis: Òun ni Orísun ìwàláàyè , Ẹni tó fún un ní ẹ̀bùn ọ̀fẹ́ nípasẹ̀ Kristi .\n",
            "2020-03-04 20:24:30,484 Example #1\n",
            "2020-03-04 20:24:30,485 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-03-04 20:24:30,485 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-03-04 20:24:30,485 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó bójú mu ni .\n",
            "2020-03-04 20:24:30,485 Example #2\n",
            "2020-03-04 20:24:30,486 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-03-04 20:24:30,486 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-03-04 20:24:30,486 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn lọ ?\n",
            "2020-03-04 20:24:30,486 Example #3\n",
            "2020-03-04 20:24:30,487 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-03-04 20:24:30,487 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-03-04 20:24:30,487 \tHypothesis: Ó ní ìrírí tó pọ̀ ju ti tẹ́lẹ̀ lọ , ó sì ní okun ju bó o ṣe ń ṣe , àmọ́ ó ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-03-04 20:24:30,487 Validation result (greedy) at epoch   8, step    37000: bleu:  26.67, loss: 42865.5039, ppl:   5.6085, duration: 65.2948s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "outputId": "a81446bd-a4ee-4a40-cb02-9ffe7aaa75b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "# Output our validation accuracy epoch 1-30\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 115148.35156\tPPL: 102.70621\tbleu: 0.00000\tLR: 0.00030000\t*\n",
            "Steps: 2000\tLoss: 97475.09375\tPPL: 50.44891\tbleu: 2.11136\tLR: 0.00030000\t*\n",
            "Steps: 3000\tLoss: 87086.53125\tPPL: 33.21756\tbleu: 3.19671\tLR: 0.00030000\t*\n",
            "Steps: 4000\tLoss: 79800.75000\tPPL: 24.77924\tbleu: 4.45802\tLR: 0.00030000\t*\n",
            "Steps: 5000\tLoss: 74459.42969\tPPL: 19.98836\tbleu: 6.20650\tLR: 0.00030000\t*\n",
            "Steps: 6000\tLoss: 70707.52344\tPPL: 17.18830\tbleu: 7.75229\tLR: 0.00030000\t*\n",
            "Steps: 7000\tLoss: 66718.93750\tPPL: 14.64043\tbleu: 9.56209\tLR: 0.00030000\t*\n",
            "Steps: 8000\tLoss: 63464.62891\tPPL: 12.84406\tbleu: 11.41239\tLR: 0.00030000\t*\n",
            "Steps: 9000\tLoss: 62754.48438\tPPL: 12.48235\tbleu: 12.00613\tLR: 0.00030000\t*\n",
            "Steps: 10000\tLoss: 58180.71875\tPPL: 10.38471\tbleu: 15.04528\tLR: 0.00030000\t*\n",
            "Steps: 11000\tLoss: 56614.98047\tPPL: 9.75083\tbleu: 16.10048\tLR: 0.00030000\t*\n",
            "Steps: 12000\tLoss: 55189.18750\tPPL: 9.20733\tbleu: 17.56142\tLR: 0.00030000\t*\n",
            "Steps: 13000\tLoss: 53802.56250\tPPL: 8.70783\tbleu: 18.58614\tLR: 0.00030000\t*\n",
            "Steps: 14000\tLoss: 52571.85156\tPPL: 8.28724\tbleu: 19.37459\tLR: 0.00030000\t*\n",
            "Steps: 15000\tLoss: 51497.78906\tPPL: 7.93681\tbleu: 19.83467\tLR: 0.00030000\t*\n",
            "Steps: 16000\tLoss: 50615.42188\tPPL: 7.66005\tbleu: 20.54956\tLR: 0.00030000\t*\n",
            "Steps: 17000\tLoss: 50131.03516\tPPL: 7.51224\tbleu: 20.27431\tLR: 0.00030000\t*\n",
            "Steps: 18000\tLoss: 49279.38281\tPPL: 7.25925\tbleu: 21.09863\tLR: 0.00030000\t*\n",
            "Steps: 19000\tLoss: 48661.05078\tPPL: 7.08092\tbleu: 21.64172\tLR: 0.00030000\t*\n",
            "Steps: 20000\tLoss: 47769.46875\tPPL: 6.83147\tbleu: 22.28391\tLR: 0.00030000\t*\n",
            "Steps: 21000\tLoss: 47556.09766\tPPL: 6.77308\tbleu: 22.79896\tLR: 0.00030000\t*\n",
            "Steps: 22000\tLoss: 48029.51562\tPPL: 6.90330\tbleu: 22.47556\tLR: 0.00030000\t\n",
            "Steps: 23000\tLoss: 46747.20312\tPPL: 6.55625\tbleu: 23.32771\tLR: 0.00030000\t*\n",
            "Steps: 24000\tLoss: 46248.17969\tPPL: 6.42595\tbleu: 23.91731\tLR: 0.00030000\t*\n",
            "Steps: 25000\tLoss: 45661.53125\tPPL: 6.27609\tbleu: 24.64429\tLR: 0.00030000\t*\n",
            "Steps: 26000\tLoss: 45456.78516\tPPL: 6.22461\tbleu: 24.03838\tLR: 0.00030000\t*\n",
            "Steps: 27000\tLoss: 45325.14844\tPPL: 6.19174\tbleu: 24.93773\tLR: 0.00030000\t*\n",
            "Steps: 28000\tLoss: 44830.29297\tPPL: 6.06971\tbleu: 25.65433\tLR: 0.00030000\t*\n",
            "Steps: 29000\tLoss: 44760.87891\tPPL: 6.05278\tbleu: 24.48872\tLR: 0.00030000\t*\n",
            "Steps: 30000\tLoss: 44387.25000\tPPL: 5.96249\tbleu: 25.06173\tLR: 0.00030000\t*\n",
            "Steps: 31000\tLoss: 44096.47266\tPPL: 5.89316\tbleu: 25.72908\tLR: 0.00030000\t*\n",
            "Steps: 32000\tLoss: 43940.62500\tPPL: 5.85633\tbleu: 26.10125\tLR: 0.00030000\t*\n",
            "Steps: 33000\tLoss: 43693.83984\tPPL: 5.79848\tbleu: 25.84444\tLR: 0.00030000\t*\n",
            "Steps: 34000\tLoss: 43429.41406\tPPL: 5.73713\tbleu: 25.78298\tLR: 0.00030000\t*\n",
            "Steps: 35000\tLoss: 43144.37891\tPPL: 5.67173\tbleu: 26.19126\tLR: 0.00030000\t*\n",
            "Steps: 36000\tLoss: 43116.25000\tPPL: 5.66532\tbleu: 25.95466\tLR: 0.00030000\t*\n",
            "Steps: 37000\tLoss: 42865.50391\tPPL: 5.60846\tbleu: 26.67443\tLR: 0.00030000\t*\n",
            "Steps: 38000\tLoss: 42954.18359\tPPL: 5.62850\tbleu: 26.30775\tLR: 0.00030000\t\n",
            "Steps: 39000\tLoss: 42797.95703\tPPL: 5.59324\tbleu: 26.77904\tLR: 0.00030000\t*\n",
            "Steps: 40000\tLoss: 42529.14453\tPPL: 5.53309\tbleu: 27.02491\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "outputId": "d509de94-4492-4e06-c13f-6a991257e315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-05 04:25:31,133 Hello! This is Joey-NMT.\n",
            "2020-03-05 04:28:31,183  dev bleu:  27.64 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-03-05 04:29:46,234 test bleu:  34.90 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}